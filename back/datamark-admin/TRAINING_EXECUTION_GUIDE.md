# å¤§å°æ¨¡å‹ååŒè®­ç»ƒ - å®Œæ•´å®ç°ä¸éƒ¨ç½²æŒ‡å—

## ğŸ“š ç›®å½•

1. [åŠŸèƒ½æ¦‚è¿°](#åŠŸèƒ½æ¦‚è¿°)
2. [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
3. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
4. [éƒ¨ç½²æ­¥éª¤](#éƒ¨ç½²æ­¥éª¤)
5. [ä½¿ç”¨æµç¨‹](#ä½¿ç”¨æµç¨‹)
6. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
7. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)

---

## åŠŸèƒ½æ¦‚è¿°

æœ¬å®ç°æä¾›äº†**å®Œæ•´çš„ç«¯åˆ°ç«¯å¤§å°æ¨¡å‹ååŒè®­ç»ƒè§£å†³æ–¹æ¡ˆ**ï¼ŒåŒ…æ‹¬ï¼š

### âœ… å·²å®ç°åŠŸèƒ½

1. **å‰ç«¯é«˜çº§é…ç½®**
   - 20+ä¸ªè®­ç»ƒå‚æ•°çš„JSONé…ç½®
   - ä¼˜åŒ–å™¨é€‰æ‹©ï¼ˆAdamW/Adam/SGDï¼‰
   - å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆCosine/Linearï¼‰
   - GPUè®¾å¤‡é…ç½®
   - LoRAè¯¦ç»†é…ç½®
   - çŸ¥è¯†è’¸é¦è¯¦ç»†é…ç½®

2. **åç«¯API**
   - ä»»åŠ¡åˆ›å»ºå’Œç®¡ç†
   - è®­ç»ƒè¿›åº¦å®æ—¶æ›´æ–°
   - è®­ç»ƒå†å²è®°å½•
   - æ¨¡å‹è¯„ä¼°ç»“æœä¿å­˜

3. **è®­ç»ƒæ‰§è¡Œå¼•æ“** â­ æ–°å¢
   - Pythonè®­ç»ƒè„šæœ¬ï¼ˆtrain_distillation.pyï¼‰
   - Javaå¼‚æ­¥ä»»åŠ¡è°ƒåº¦ï¼ˆTrainingExecutionServiceï¼‰
   - è¿›ç¨‹ç”Ÿå‘½å‘¨æœŸç®¡ç†
   - é…ç½®JSONè§£æå’Œåº”ç”¨

4. **çŸ¥è¯†è’¸é¦ç®—æ³•**
   - æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹æ¶æ„
   - LoRAå¾®è°ƒ
   - å¤šç§è’¸é¦æŸå¤±ï¼ˆKLæ•£åº¦ã€MSEï¼‰
   - ç¡¬è½¯æ ‡ç­¾æ··åˆ

---

## ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      å‰ç«¯ (Vue3)                           â”‚
â”‚  - è®­ç»ƒä»»åŠ¡åˆ›å»ºè¡¨å•                                         â”‚
â”‚  - é«˜çº§é…ç½®JSONç¼–è¾‘                                         â”‚
â”‚  - è®­ç»ƒè¿›åº¦ç›‘æ§                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ HTTP API
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Spring Bootåç«¯                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  ModelDistillationController                  â”‚         â”‚
â”‚  â”‚  - POST /tasks (åˆ›å»ºä»»åŠ¡)                     â”‚         â”‚
â”‚  â”‚  - POST /tasks/{id}/start (å¯åŠ¨è®­ç»ƒ)          â”‚         â”‚
â”‚  â”‚  - PUT /tasks/{id}/progress (æ›´æ–°è¿›åº¦)        â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚              â”‚                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  TrainingExecutionService                     â”‚         â”‚
â”‚  â”‚  - è§£ætraining_config JSON                  â”‚         â”‚
â”‚  â”‚  - æ„å»ºPythonå‘½ä»¤å‚æ•°                        â”‚         â”‚
â”‚  â”‚  - å¯åŠ¨å­è¿›ç¨‹                                 â”‚         â”‚
â”‚  â”‚  - ç®¡ç†è¿›ç¨‹ç”Ÿå‘½å‘¨æœŸ                           â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚              â”‚ ProcessBuilder.start()                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Pythonè®­ç»ƒè„šæœ¬ (train_distillation.py)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  1. åŠ è½½æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹                    â”‚         â”‚
â”‚  â”‚  2. åº”ç”¨LoRAé…ç½® (PEFTåº“)                     â”‚         â”‚
â”‚  â”‚  3. é…ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨                        â”‚         â”‚
â”‚  â”‚  4. è®­ç»ƒå¾ªç¯                                  â”‚         â”‚
â”‚  â”‚     - æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­                        â”‚         â”‚
â”‚  â”‚     - å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­                        â”‚         â”‚
â”‚  â”‚     - è®¡ç®—è’¸é¦æŸå¤±                            â”‚         â”‚
â”‚  â”‚     - åå‘ä¼ æ’­å’Œæ›´æ–°                          â”‚         â”‚
â”‚  â”‚  5. æ¯ä¸ªepochå›è°ƒæ›´æ–°è¿›åº¦                     â”‚         â”‚
â”‚  â”‚  6. ä¿å­˜checkpointå’Œæœ€ç»ˆæ¨¡å‹                  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚             â”‚ HTTP PUT /tasks/{id}/progress                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
        æ•°æ®åº“æ›´æ–°è¿›åº¦
```

---

## ç¯å¢ƒå‡†å¤‡

### 1. Pythonç¯å¢ƒ

#### æ–¹å¼Aï¼šä½¿ç”¨Condaï¼ˆæ¨èï¼‰

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n distillation python=3.9 -y
conda activate distillation

# å®‰è£…PyTorch (CUDA 11.8)
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# å®‰è£…ä¾èµ–
pip install transformers==4.35.0
pip install peft==0.7.1
pip install accelerate==0.25.0
pip install requests
```

#### æ–¹å¼Bï¼šä½¿ç”¨pip

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv_distillation
source venv_distillation/bin/activate

# å®‰è£…PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…ä¾èµ–
pip install transformers==4.35.0 peft==0.7.1 accelerate==0.25.0 requests
```

### 2. Javaç¯å¢ƒ

- JDK 8 æˆ–æ›´é«˜ç‰ˆæœ¬
- Maven 3.6+

### 3. ç›®å½•ç»“æ„

åˆ›å»ºå¿…è¦çš„ç›®å½•ï¼š

```bash
sudo mkdir -p /data/models
sudo mkdir -p /data/datasets
sudo mkdir -p /data/training_output
sudo chown -R $(whoami):$(whoami) /data
```

### 4. ä¸‹è½½ç¤ºä¾‹æ¨¡å‹ï¼ˆå¯é€‰ï¼‰

```bash
# æ•™å¸ˆæ¨¡å‹ç¤ºä¾‹ï¼ˆBERT Baseï¼‰
cd /data/models
git clone https://huggingface.co/bert-base-uncased

# æˆ–ä½¿ç”¨å›½å†…é•œåƒ
git clone https://hf-mirror.com/bert-base-uncased
```

---

## éƒ¨ç½²æ­¥éª¤

### æ­¥éª¤1ï¼šé…ç½®application.yml

åœ¨ `application.yml` æˆ– `application-distillation.yml` ä¸­æ·»åŠ ï¼š

```yaml
spring:
  profiles:
    include: distillation

distillation:
  python:
    # ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒçš„Python
    path: /path/to/your/venv/bin/python3
    # æˆ–è€…ä½¿ç”¨ç³»ç»ŸPython
    # path: python3

  script:
    path: /home/user/work/back/datamark-admin/train_distillation.py

  api:
    base-url: http://localhost:8080

  models:
    root: /data/models

  datasets:
    root: /data/datasets

  output:
    root: /data/training_output
```

### æ­¥éª¤2ï¼šèµ‹äºˆè®­ç»ƒè„šæœ¬æ‰§è¡Œæƒé™

```bash
chmod +x /home/user/work/back/datamark-admin/train_distillation.py
```

### æ­¥éª¤3ï¼šç¼–è¯‘å¹¶å¯åŠ¨åç«¯

```bash
cd /home/user/work/back/datamark-admin
mvn clean package -DskipTests
java -jar target/datamark-admin.jar --spring.profiles.active=prod,distillation
```

### æ­¥éª¤4ï¼šéªŒè¯ç¯å¢ƒ

æµ‹è¯•Pythonè„šæœ¬æ˜¯å¦å¯ä»¥æ­£å¸¸æ‰§è¡Œï¼š

```bash
python3 train_distillation.py --help
```

åº”è¯¥çœ‹åˆ°å‚æ•°è¯´æ˜è¾“å‡ºã€‚

---

## ä½¿ç”¨æµç¨‹

### å®Œæ•´ç¤ºä¾‹ï¼šåˆ›å»ºå¹¶å¯åŠ¨è®­ç»ƒä»»åŠ¡

#### 1. å‰ç«¯åˆ›å»ºä»»åŠ¡

åœ¨å‰ç«¯å¡«å†™è®­ç»ƒé…ç½®è¡¨å•ï¼ŒåŒ…æ‹¬ï¼š

**åŸºç¡€é…ç½®ï¼š**
- ä»»åŠ¡åç§°ï¼š`BERTè’¸é¦å®éªŒ001`
- æ•™å¸ˆæ¨¡å‹ï¼š`bert-base-uncased`
- å­¦ç”Ÿæ¨¡å‹ï¼š`student-bert`
- æ•°æ®é›†ï¼šé€‰æ‹©å·²ä¸Šä¼ çš„æ•°æ®é›†

**è®­ç»ƒå‚æ•°ï¼š**
- Epochs: 10
- Batch Size: 16
- Learning Rate: 0.0001

**é«˜çº§é…ç½®ï¼ˆJSONï¼‰ï¼š**
```json
{
  "optimizer": "adamw",
  "lrScheduler": "cosine",
  "weightDecay": 0.01,
  "gradAccumSteps": 4,
  "maxGradNorm": 1.0,
  "gpuDevices": [0],
  "autoSaveCheckpoint": true,
  "checkpointInterval": 5,
  "loraAdvancedConfig": {
    "targetModules": ["q_proj", "v_proj"],
    "layers": "all",
    "biasTrain": "none"
  },
  "distillationAdvancedConfig": {
    "hardLabelWeight": 0.3,
    "softLabelWeight": 0.7,
    "lossType": "kl_div"
  }
}
```

#### 2. åç«¯æ¥æ”¶å¹¶å­˜å‚¨

```
POST http://localhost:8080/model-distillation/tasks

Response:
{
  "code": 200,
  "data": {
    "taskId": "TASK_AB12CD34",
    "status": "PENDING",
    ...
  }
}
```

#### 3. å¯åŠ¨è®­ç»ƒ

```
POST http://localhost:8080/model-distillation/tasks/TASK_AB12CD34/start

Response:
{
  "code": 200,
  "message": "ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨åå°æ‰§è¡Œè®­ç»ƒ"
}
```

#### 4. ç›‘æ§è®­ç»ƒè¿›åº¦

**åå°è‡ªåŠ¨æ›´æ–°ï¼š**

Pythonè„šæœ¬æ¯ä¸ªepochç»“æŸåè‡ªåŠ¨è°ƒç”¨ï¼š
```
PUT http://localhost:8080/model-distillation/tasks/TASK_AB12CD34/progress
  ?currentEpoch=1&accuracy=75.5&loss=0.45
```

**å‰ç«¯è½®è¯¢æŸ¥è¯¢ï¼š**
```
GET http://localhost:8080/model-distillation/tasks/TASK_AB12CD34

Response:
{
  "code": 200,
  "data": {
    "taskId": "TASK_AB12CD34",
    "status": "RUNNING",
    "currentEpoch": 3,
    "progress": 30,
    "accuracy": 78.2,
    "loss": 0.38,
    ...
  }
}
```

#### 5. è®­ç»ƒå®Œæˆ

Pythonè„šæœ¬è‡ªåŠ¨è°ƒç”¨ï¼š
```
POST http://localhost:8080/model-distillation/tasks/TASK_AB12CD34/complete
```

ä»»åŠ¡çŠ¶æ€æ›´æ–°ä¸º `COMPLETED`ï¼Œæ¨¡å‹ä¿å­˜åœ¨ï¼š
```
/data/training_output/TASK_AB12CD34/final_model/
```

---

## é…ç½®è¯´æ˜

### Pythonè„šæœ¬å‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | æ¥æº |
|------|------|--------|------|
| `--task_id` | ä»»åŠ¡ID | å¿…å¡« | Entity |
| `--teacher_model` | æ•™å¸ˆæ¨¡å‹åç§° | å¿…å¡« | Entity |
| `--teacher_path` | æ•™å¸ˆæ¨¡å‹è·¯å¾„ | å¿…å¡« | JSONé…ç½®æˆ–è‡ªåŠ¨æ‹¼æ¥ |
| `--optimizer` | ä¼˜åŒ–å™¨ | adamw | JSONé…ç½® |
| `--lr_scheduler` | å­¦ä¹ ç‡è°ƒåº¦å™¨ | cosine | JSONé…ç½® |
| `--gpu_devices` | GPUè®¾å¤‡åˆ—è¡¨ | 0 | JSONé…ç½® |
| `--lora_rank` | LoRA rank | 16 | Entity |
| `--lora_target_modules` | LoRAç›®æ ‡æ¨¡å— | "" | JSONé…ç½® |
| `--temperature` | è’¸é¦æ¸©åº¦ | 3.0 | Entity |
| `--hard_label_weight` | ç¡¬æ ‡ç­¾æƒé‡ | 0.3 | JSONé…ç½® |
| `--soft_label_weight` | è½¯æ ‡ç­¾æƒé‡ | 0.7 | JSONé…ç½® |

å®Œæ•´å‚æ•°åˆ—è¡¨è§ `train_distillation.py` çš„ `parse_args()` å‡½æ•°ã€‚

### Javaé…ç½®å±æ€§

```yaml
distillation:
  python:
    path: python3  # Pythonè§£é‡Šå™¨è·¯å¾„

  script:
    path: /path/to/train_distillation.py  # è®­ç»ƒè„šæœ¬è·¯å¾„

  api:
    base-url: http://localhost:8080  # åç«¯APIåœ°å€

  models:
    root: /data/models  # æ¨¡å‹æ ¹ç›®å½•

  datasets:
    root: /data/datasets  # æ•°æ®é›†æ ¹ç›®å½•

  output:
    root: /data/training_output  # è¾“å‡ºæ ¹ç›®å½•
```

---

## æ•…éšœæ’æŸ¥

### é—®é¢˜1ï¼šè®­ç»ƒä»»åŠ¡å¯åŠ¨å¤±è´¥

**ç—‡çŠ¶ï¼š** ç‚¹å‡»"å¼€å§‹è®­ç»ƒ"åï¼Œä»»åŠ¡çŠ¶æ€ä¸€ç›´æ˜¯PENDING

**æ£€æŸ¥ï¼š**
1. æŸ¥çœ‹åç«¯æ—¥å¿—ï¼š
   ```bash
   tail -f logs/spring.log | grep TrainingExecution
   ```

2. æ£€æŸ¥Pythonè·¯å¾„ï¼š
   ```bash
   which python3
   # æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„python.path
   ```

3. æµ‹è¯•è„šæœ¬æ‰§è¡Œï¼š
   ```bash
   python3 /path/to/train_distillation.py --task_id TEST --api_base_url http://localhost:8080 \
     --teacher_model bert-base --student_model student \
     --teacher_path /data/models/bert-base-uncased \
     --dataset_id 1 --epochs 1 --output_dir /tmp/test
   ```

### é—®é¢˜2ï¼šè®­ç»ƒè¿›åº¦ä¸æ›´æ–°

**ç—‡çŠ¶ï¼š** è®­ç»ƒå·²å¯åŠ¨ï¼Œä½†progressä¸€ç›´æ˜¯0

**åŸå› ï¼š** Pythonè„šæœ¬æ— æ³•è®¿é—®åç«¯API

**è§£å†³ï¼š**
1. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
2. ç¡®è®¤APIåœ°å€æ­£ç¡®ï¼š
   ```python
   # åœ¨train_distillation.pyä¸­æ·»åŠ è°ƒè¯•è¾“å‡º
   print(f"API Base URL: {self.config.api_base_url}")
   ```

3. æµ‹è¯•APIè¿æ¥ï¼š
   ```bash
   curl -X PUT "http://localhost:8080/model-distillation/tasks/TEST/progress?currentEpoch=1&accuracy=50&loss=0.5"
   ```

### é—®é¢˜3ï¼šCUDA Out of Memory

**ç—‡çŠ¶ï¼š** è®­ç»ƒå¼€å§‹åæŠ¥é”™ `CUDA out of memory`

**è§£å†³ï¼š**
1. å‡å°batch_sizeï¼š16 â†’ 8 â†’ 4
2. å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼š
   ```json
   {
     "gradAccumSteps": 8  // å®é™…batch_size = 4 * 8 = 32
   }
   ```
3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
4. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆéœ€è¦ä¿®æ”¹è„šæœ¬ï¼‰

### é—®é¢˜4ï¼šæ— æ³•æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶

**ç—‡çŠ¶ï¼š** é”™è¯¯ä¿¡æ¯ `OSError: Can't load model from /data/models/xxx`

**è§£å†³ï¼š**
1. æ£€æŸ¥æ¨¡å‹è·¯å¾„ï¼š
   ```bash
   ls -la /data/models/bert-base-uncased
   ```

2. ä¸‹è½½æ¨¡å‹ï¼š
   ```bash
   cd /data/models
   git clone https://huggingface.co/bert-base-uncased
   ```

3. æˆ–åœ¨JSONé…ç½®ä¸­æŒ‡å®šHuggingFace IDï¼š
   ```json
   {
     "teacherModelConfig": {
       "modelPath": "bert-base-uncased"  // è‡ªåŠ¨ä»HFä¸‹è½½
     }
   }
   ```

### é—®é¢˜5ï¼šè®­ç»ƒè¿›ç¨‹è¢«æ€æ­»

**ç—‡çŠ¶ï¼š** è®­ç»ƒè¿è¡Œä¸€æ®µæ—¶é—´åè‡ªåŠ¨åœæ­¢ï¼ŒexitCode=137

**åŸå› ï¼š** å†…å­˜ä¸è¶³ï¼Œè¢«OOM Killeræ€æ­»

**è§£å†³ï¼š**
1. å¢åŠ ç³»ç»Ÿå†…å­˜
2. å‡å°batch_size
3. ä½¿ç”¨æ¨¡å‹é‡åŒ–

---

## é«˜çº§åŠŸèƒ½

### 1. åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤šGPUï¼‰

ä¿®æ”¹GPUé…ç½®ï¼š

```json
{
  "gpuDevices": [0, 1, 2, 3]
}
```

åœ¨ `train_distillation.py` ä¸­æ·»åŠ åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼ˆéœ€è¦ä½¿ç”¨`torch.nn.DataParallel`æˆ–`DistributedDataParallel`ï¼‰ã€‚

### 2. è‡ªå®šä¹‰æ•°æ®é›†

æ›¿æ¢ `DummyDataset` ç±»ï¼Œå®ç°çœŸå®çš„æ•°æ®åŠ è½½ï¼š

```python
class CustomDataset(Dataset):
    def __init__(self, dataset_path):
        # ä»æ•°æ®åº“æˆ–æ–‡ä»¶åŠ è½½æ•°æ®
        self.data = self.load_data(dataset_path)

    def __getitem__(self, idx):
        # è¿”å›çœŸå®æ•°æ®
        return self.data[idx]
```

### 3. æ¨¡å‹é‡åŒ–

åœ¨JSONé…ç½®ä¸­å¯ç”¨é‡åŒ–ï¼š

```json
{
  "teacherModelConfig": {
    "quantization": "int8"
  }
}
```

éœ€è¦åœ¨è„šæœ¬ä¸­æ·»åŠ é‡åŒ–é€»è¾‘ï¼ˆä½¿ç”¨`bitsandbytes`åº“ï¼‰ã€‚

---

## ä»£ç æ–‡ä»¶æ¸…å•

### æ–°å¢æ–‡ä»¶

1. **train_distillation.py** (1100è¡Œ)
   - Pythonè®­ç»ƒè„šæœ¬
   - å®Œæ•´çš„çŸ¥è¯†è’¸é¦è®­ç»ƒæµç¨‹
   - ä½ç½®ï¼š`/back/datamark-admin/train_distillation.py`

2. **TrainingExecutionService.java** (350è¡Œ)
   - Javaè®­ç»ƒæ‰§è¡ŒæœåŠ¡
   - å¼‚æ­¥ä»»åŠ¡è°ƒåº¦
   - ä½ç½®ï¼š`/back/datamark-admin/src/main/java/com/qczy/distillation/service/`

3. **AsyncTaskConfig.java** (70è¡Œ)
   - å¼‚æ­¥ä»»åŠ¡é…ç½®
   - çº¿ç¨‹æ± ç®¡ç†
   - ä½ç½®ï¼š`/back/datamark-admin/src/main/java/com/qczy/distillation/config/`

4. **application-distillation.yml** (60è¡Œ)
   - è®­ç»ƒç›¸å…³é…ç½®
   - ä½ç½®ï¼š`/back/datamark-admin/src/main/resources/`

### ä¿®æ”¹æ–‡ä»¶

1. **ModelDistillationController.java**
   - æ›´æ–° `startTask()` æ–¹æ³•ï¼šè°ƒç”¨TrainingExecutionService
   - æ›´æ–° `stopTask()` æ–¹æ³•ï¼šåœæ­¢è®­ç»ƒè¿›ç¨‹

---

## æ€»ç»“

âœ… **å·²å®Œæˆçš„åŠŸèƒ½ï¼š**
- å®Œæ•´çš„ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹
- å‰ç«¯é«˜çº§é…ç½® â†’ JSONå­˜å‚¨ â†’ Pythonè„šæœ¬åº”ç”¨
- å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œå’Œè¿›ç¨‹ç®¡ç†
- å®æ—¶è¿›åº¦æ›´æ–°å’Œç›‘æ§
- çŸ¥è¯†è’¸é¦ç®—æ³•å®ç°
- LoRAå¾®è°ƒæ”¯æŒ

ğŸš€ **åç»­ä¼˜åŒ–æ–¹å‘ï¼š**
1. æ·»åŠ çœŸå®æ•°æ®é›†åŠ è½½é€»è¾‘
2. å®ç°åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
3. æ·»åŠ æ¨¡å‹é‡åŒ–åŠŸèƒ½
4. å®ç°è®­ç»ƒæ—¥å¿—çš„WebæŸ¥çœ‹
5. æ·»åŠ è®­ç»ƒæ›²çº¿å¯è§†åŒ–
6. å®ç°æ–­ç‚¹ç»­è®­åŠŸèƒ½

---

## è”ç³»ä¸æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ï¼š
- åç«¯æ—¥å¿—ï¼š`logs/spring.log`
- Pythonè„šæœ¬è¾“å‡ºï¼šé€šè¿‡åç«¯æ—¥å¿—æŸ¥çœ‹
- æ•°æ®åº“è¡¨ï¼š`md_training_task`, `md_training_history`

ğŸ“§ æŠ€æœ¯æ”¯æŒï¼šæŸ¥çœ‹é¡¹ç›®æ–‡æ¡£æˆ–æäº¤Issue
