#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen2.5-VL åˆ°å¤šæ¶æ„å°æ¨¡å‹çš„çŸ¥è¯†è’¸é¦è®­ç»ƒè„šæœ¬

æ”¯æŒçš„å­¦ç”Ÿæ¨¡å‹ï¼š
1. LSTM - åºåˆ—ç‰¹å¾æå–
2. UNet - å›¾åƒåˆ†å‰²
3. YOLOv8 - ç›®æ ‡æ£€æµ‹
4. ResNet - å›¾åƒåˆ†ç±»
5. Vision Transformer - å›¾åƒåˆ†ç±»

è’¸é¦ç­–ç•¥ï¼š
- ç‰¹å¾è’¸é¦ï¼šæå–Qwen2.5-VLçš„è§†è§‰ç¼–ç å™¨ç‰¹å¾
- Logitsè’¸é¦ï¼šç”¨äºåˆ†ç±»ä»»åŠ¡
- ä¸­é—´å±‚è’¸é¦ï¼šç”¨äºViTç­‰Transformeræ¶æ„

ä½œè€…ï¼šClaude Assistant
æ—¥æœŸï¼š2026-01-11
"""

import argparse
import json
import os
import sys
import time
import warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import numpy as np
import requests
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
from tqdm import tqdm

# Qwen2.5-VLç›¸å…³å¯¼å…¥
try:
    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
    QWEN_AVAILABLE = True
except ImportError:
    QWEN_AVAILABLE = False
    warnings.warn("Qwen2VLæ¨¡å‹åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

# å°æ¨¡å‹ç›¸å…³å¯¼å…¥
import torchvision.models as models
from transformers import ViTForImageClassification, ViTImageProcessor

# YOLOç›¸å…³ï¼ˆéœ€è¦å®‰è£…ultralyticsï¼‰
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    warnings.warn("YOLOv8æœªå®‰è£…")

# ==================== é…ç½®ç±» ====================

class TrainingConfig:
    """å¤šæ¨¡å‹ååŒè®­ç»ƒé…ç½®"""

    def __init__(self, args):
        # åŸºç¡€é…ç½®
        self.task_id = args.task_id
        self.api_base_url = args.api_base_url

        # æ•™å¸ˆæ¨¡å‹é…ç½®ï¼ˆQwen2.5-VLï¼‰
        self.teacher_model_path = args.teacher_model_path
        self.teacher_model_type = "qwen2.5-vl"  # å›ºå®šä¸ºQwen2.5-VL

        # å­¦ç”Ÿæ¨¡å‹é…ç½®
        self.student_model_type = args.student_model_type  # lstm/unet/yolov8/resnet/vit
        self.student_model_path = args.student_model_path
        self.student_model_size = args.student_model_size  # å¦‚resnet18/resnet50, vit-baseç­‰

        # ä»»åŠ¡é…ç½®
        self.task_type = args.task_type  # classification/detection/segmentation
        self.num_classes = args.num_classes

        # æ•°æ®é…ç½®
        self.dataset_path = args.dataset_path
        self.val_dataset_path = args.val_dataset_path
        self.image_size = args.image_size

        # è®­ç»ƒè¶…å‚æ•°
        self.epochs = args.epochs
        self.batch_size = args.batch_size
        self.learning_rate = args.learning_rate
        self.optimizer_type = args.optimizer_type  # adamw/adam/sgd
        self.lr_scheduler = args.lr_scheduler  # cosine/linear/step
        self.weight_decay = args.weight_decay
        self.grad_accum_steps = args.grad_accum_steps
        self.max_grad_norm = args.max_grad_norm

        # è’¸é¦é…ç½®
        self.distillation_type = args.distillation_type  # feature/logit/layer/hybrid
        self.temperature = args.temperature
        self.alpha = args.alpha  # ç¡¬æ ‡ç­¾æƒé‡
        self.beta = args.beta   # è½¯æ ‡ç­¾æƒé‡
        self.gamma = args.gamma  # ç‰¹å¾è’¸é¦æƒé‡

        # ç‰¹å¾è’¸é¦é…ç½®
        self.feature_loss_type = args.feature_loss_type  # mse/cosine/attention
        self.align_feature = args.align_feature  # æ˜¯å¦ä½¿ç”¨æŠ•å½±å±‚å¯¹é½ç‰¹å¾
        self.feature_dim = args.feature_dim  # ç‰¹å¾å¯¹é½ç»´åº¦

        # GPUé…ç½®
        self.gpu_devices = self._parse_gpu_devices(args.gpu_devices)
        self.use_amp = args.use_amp  # æ··åˆç²¾åº¦è®­ç»ƒ

        # è¾“å‡ºé…ç½®
        self.output_dir = args.output_dir
        self.checkpoint_interval = args.checkpoint_interval
        self.log_interval = args.log_interval

    def _parse_gpu_devices(self, gpu_str: str) -> List[int]:
        """è§£æGPUè®¾å¤‡åˆ—è¡¨"""
        if not gpu_str or gpu_str == "":
            return [0]
        return [int(x.strip()) for x in gpu_str.split(",")]


# ==================== æ•°æ®é›†ç±» ====================

class MultiTaskDataset(Dataset):
    """
    å¤šä»»åŠ¡æ•°æ®é›†ï¼Œæ”¯æŒåˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ä»»åŠ¡
    """

    def __init__(
        self,
        dataset_path: str,
        task_type: str,
        image_size: int = 224,
        num_classes: int = 10,
        mode: str = 'train'
    ):
        self.dataset_path = Path(dataset_path)
        self.task_type = task_type
        self.image_size = image_size
        self.num_classes = num_classes
        self.mode = mode

        # æ•°æ®å¢å¼º
        if mode == 'train':
            self.transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.RandomHorizontalFlip(),
                transforms.RandomRotation(15),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
        else:
            self.transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])

        # TODO: ä»æ•°æ®åº“åŠ è½½çœŸå®æ•°æ®
        self.num_samples = 1000 if mode == 'train' else 200

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # TODO: åŠ è½½çœŸå®å›¾åƒå’Œæ ‡æ³¨
        # è¿™é‡Œä½¿ç”¨éšæœºæ•°æ®ä½œä¸ºæ¼”ç¤º

        # ç”Ÿæˆéšæœºå›¾åƒ
        image_array = np.random.randint(0, 255, (self.image_size, self.image_size, 3), dtype=np.uint8)
        image = Image.fromarray(image_array)
        pixel_values = self.transform(image)

        # æ ¹æ®ä»»åŠ¡ç±»å‹ç”Ÿæˆæ ‡ç­¾
        if self.task_type == 'classification':
            # åˆ†ç±»ï¼šç±»åˆ«æ ‡ç­¾
            label = torch.randint(0, self.num_classes, (1,)).item()
            return {'pixel_values': pixel_values, 'labels': label}

        elif self.task_type == 'detection':
            # æ£€æµ‹ï¼šè¾¹ç•Œæ¡† + ç±»åˆ«
            num_boxes = np.random.randint(1, 5)
            boxes = torch.rand(num_boxes, 4)  # [x1, y1, x2, y2]
            labels = torch.randint(0, self.num_classes, (num_boxes,))
            return {
                'pixel_values': pixel_values,
                'boxes': boxes,
                'labels': labels
            }

        elif self.task_type == 'segmentation':
            # åˆ†å‰²ï¼šåƒç´ çº§æ ‡ç­¾
            mask = torch.randint(0, self.num_classes, (self.image_size, self.image_size))
            return {'pixel_values': pixel_values, 'mask': mask}

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {self.task_type}")


# ==================== æ¨¡å‹åŠ è½½å™¨ ====================

class TeacherModelLoader:
    """Qwen2.5-VLæ•™å¸ˆæ¨¡å‹åŠ è½½å™¨"""

    @staticmethod
    def load_qwen2vl(model_path: str, device: torch.device):
        """åŠ è½½Qwen2.5-VLæ¨¡å‹"""
        if not QWEN_AVAILABLE:
            print("âš ï¸  Qwen2VLæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹")
            return None, None

        print(f"æ­£åœ¨åŠ è½½Qwen2.5-VLæ•™å¸ˆæ¨¡å‹: {model_path}")

        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map=device
        )
        processor = AutoProcessor.from_pretrained(model_path)

        model.eval()
        for param in model.parameters():
            param.requires_grad = False

        print(f"âœ“ Qwen2.5-VLåŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        return model, processor


class StudentModelLoader:
    """å­¦ç”Ÿæ¨¡å‹åŠ è½½å™¨"""

    @staticmethod
    def load_model(
        model_type: str,
        model_size: str,
        num_classes: int,
        device: torch.device,
        pretrained: bool = True
    ):
        """æ ¹æ®ç±»å‹åŠ è½½å­¦ç”Ÿæ¨¡å‹"""

        print(f"\næ­£åœ¨åŠ è½½å­¦ç”Ÿæ¨¡å‹: {model_type}-{model_size}")

        if model_type == 'resnet':
            return StudentModelLoader._load_resnet(model_size, num_classes, device, pretrained)
        elif model_type == 'vit':
            return StudentModelLoader._load_vit(model_size, num_classes, device, pretrained)
        elif model_type == 'yolov8':
            return StudentModelLoader._load_yolov8(model_size, num_classes, device)
        elif model_type == 'unet':
            return StudentModelLoader._load_unet(model_size, num_classes, device)
        elif model_type == 'lstm':
            return StudentModelLoader._load_lstm(model_size, num_classes, device)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å­¦ç”Ÿæ¨¡å‹ç±»å‹: {model_type}")

    @staticmethod
    def _load_resnet(size: str, num_classes: int, device, pretrained: bool):
        """åŠ è½½ResNetæ¨¡å‹"""
        resnet_variants = {
            'resnet18': models.resnet18,
            'resnet34': models.resnet34,
            'resnet50': models.resnet50,
            'resnet101': models.resnet101,
        }

        if size not in resnet_variants:
            raise ValueError(f"ä¸æ”¯æŒçš„ResNetå˜ä½“: {size}")

        model = resnet_variants[size](pretrained=pretrained)

        # ä¿®æ”¹æœ€åä¸€å±‚ä»¥é€‚åº”ç±»åˆ«æ•°
        in_features = model.fc.in_features
        model.fc = nn.Linear(in_features, num_classes)

        model.to(device)
        print(f"âœ“ ResNet-{size}åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        return model

    @staticmethod
    def _load_vit(size: str, num_classes: int, device, pretrained: bool):
        """åŠ è½½Vision Transformeræ¨¡å‹"""
        vit_models = {
            'vit-base': 'google/vit-base-patch16-224',
            'vit-large': 'google/vit-large-patch16-224',
            'vit-tiny': 'WinKawaks/vit-tiny-patch16-224',
        }

        if size not in vit_models:
            raise ValueError(f"ä¸æ”¯æŒçš„ViTå˜ä½“: {size}")

        model_name = vit_models[size]
        model = ViTForImageClassification.from_pretrained(
            model_name,
            num_labels=num_classes,
            ignore_mismatched_sizes=True
        )

        model.to(device)
        print(f"âœ“ ViT-{size}åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        return model

    @staticmethod
    def _load_yolov8(size: str, num_classes: int, device):
        """åŠ è½½YOLOv8æ¨¡å‹"""
        if not YOLO_AVAILABLE:
            raise ImportError("YOLOv8æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install ultralytics")

        # YOLOv8æ¨¡å‹å¤§å°
        yolo_sizes = {'n': 'yolov8n.pt', 's': 'yolov8s.pt', 'm': 'yolov8m.pt',
                     'l': 'yolov8l.pt', 'x': 'yolov8x.pt'}

        if size not in yolo_sizes:
            raise ValueError(f"ä¸æ”¯æŒçš„YOLOå¤§å°: {size}")

        model = YOLO(yolo_sizes[size])
        print(f"âœ“ YOLOv8-{size}åŠ è½½æˆåŠŸ")
        return model

    @staticmethod
    def _load_unet(size: str, num_classes: int, device):
        """åŠ è½½UNetæ¨¡å‹ï¼ˆç”¨äºåˆ†å‰²ï¼‰"""
        # ç®€å•çš„UNetå®ç°
        class SimpleUNet(nn.Module):
            def __init__(self, in_channels=3, num_classes=10):
                super().__init__()
                # ç¼–ç å™¨
                self.enc1 = self._conv_block(in_channels, 64)
                self.enc2 = self._conv_block(64, 128)
                self.enc3 = self._conv_block(128, 256)
                self.enc4 = self._conv_block(256, 512)

                # è§£ç å™¨
                self.dec3 = self._conv_block(512 + 256, 256)
                self.dec2 = self._conv_block(256 + 128, 128)
                self.dec1 = self._conv_block(128 + 64, 64)

                self.final = nn.Conv2d(64, num_classes, 1)
                self.pool = nn.MaxPool2d(2)
                self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

            def _conv_block(self, in_ch, out_ch):
                return nn.Sequential(
                    nn.Conv2d(in_ch, out_ch, 3, padding=1),
                    nn.BatchNorm2d(out_ch),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(out_ch, out_ch, 3, padding=1),
                    nn.BatchNorm2d(out_ch),
                    nn.ReLU(inplace=True)
                )

            def forward(self, x):
                # ç¼–ç 
                e1 = self.enc1(x)
                e2 = self.enc2(self.pool(e1))
                e3 = self.enc3(self.pool(e2))
                e4 = self.enc4(self.pool(e3))

                # è§£ç 
                d3 = self.dec3(torch.cat([self.upsample(e4), e3], dim=1))
                d2 = self.dec2(torch.cat([self.upsample(d3), e2], dim=1))
                d1 = self.dec1(torch.cat([self.upsample(d2), e1], dim=1))

                return self.final(d1)

        model = SimpleUNet(in_channels=3, num_classes=num_classes)
        model.to(device)
        print(f"âœ“ UNetåŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        return model

    @staticmethod
    def _load_lstm(size: str, num_classes: int, device):
        """åŠ è½½LSTMæ¨¡å‹ï¼ˆç”¨äºåºåˆ—ç‰¹å¾æå–+åˆ†ç±»ï¼‰"""
        class LSTMClassifier(nn.Module):
            def __init__(self, input_size=2048, hidden_size=512, num_layers=2, num_classes=10):
                super().__init__()
                # å°†å›¾åƒç‰¹å¾è§†ä¸ºåºåˆ—
                self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                                   batch_first=True, bidirectional=True)
                self.fc = nn.Linear(hidden_size * 2, num_classes)
                self.dropout = nn.Dropout(0.5)

                # ç”¨äºæå–å›¾åƒç‰¹å¾çš„CNN
                resnet = models.resnet50(pretrained=True)
                self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])

            def forward(self, x):
                # æå–ç‰¹å¾
                batch_size = x.size(0)
                features = self.feature_extractor(x)
                features = features.view(batch_size, -1, 1)  # [B, C, 1]
                features = features.transpose(1, 2)  # [B, 1, C]

                # LSTMå¤„ç†
                lstm_out, _ = self.lstm(features)
                lstm_out = lstm_out[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
                lstm_out = self.dropout(lstm_out)

                # åˆ†ç±»
                output = self.fc(lstm_out)
                return output

        hidden_sizes = {'small': 256, 'medium': 512, 'large': 1024}
        hidden_size = hidden_sizes.get(size, 512)

        model = LSTMClassifier(hidden_size=hidden_size, num_classes=num_classes)
        model.to(device)
        print(f"âœ“ LSTMåŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        return model


# ==================== ç‰¹å¾æŠ•å½±å±‚ ====================

class FeatureAlignmentLayer(nn.Module):
    """ç‰¹å¾å¯¹é½å±‚ï¼šå°†æ•™å¸ˆæ¨¡å‹ç‰¹å¾æŠ•å½±åˆ°å­¦ç”Ÿæ¨¡å‹ç‰¹å¾ç©ºé—´"""

    def __init__(self, teacher_dim: int, student_dim: int, use_attention: bool = False):
        super().__init__()
        self.use_attention = use_attention

        # ç®€å•æŠ•å½±å±‚
        self.projection = nn.Sequential(
            nn.Linear(teacher_dim, student_dim),
            nn.LayerNorm(student_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        # å¯é€‰ï¼šæ³¨æ„åŠ›æœºåˆ¶
        if use_attention:
            self.attention = nn.MultiheadAttention(student_dim, num_heads=8, batch_first=True)

    def forward(self, teacher_features, student_features=None):
        """
        Args:
            teacher_features: [B, N, D_teacher]
            student_features: [B, M, D_student] (å¯é€‰ï¼Œç”¨äºæ³¨æ„åŠ›)
        Returns:
            aligned_features: [B, N, D_student]
        """
        # æŠ•å½±
        aligned = self.projection(teacher_features)

        # å¯é€‰ï¼šä½¿ç”¨æ³¨æ„åŠ›è¿›è¡Œè¿›ä¸€æ­¥å¯¹é½
        if self.use_attention and student_features is not None:
            aligned, _ = self.attention(aligned, student_features, student_features)

        return aligned


# ==================== çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨ ====================

class MultiModelDistillationTrainer:
    """å¤šæ¨¡å‹ååŒè®­ç»ƒå™¨ - Qwen2.5-VL â†’ å¤šç§å°æ¨¡å‹"""

    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = self._setup_device()

        # åŠ è½½æ•™å¸ˆæ¨¡å‹ï¼ˆQwen2.5-VLï¼‰
        self.teacher_model, self.teacher_processor = TeacherModelLoader.load_qwen2vl(
            config.teacher_model_path,
            self.device
        )

        # åŠ è½½å­¦ç”Ÿæ¨¡å‹
        self.student_model = StudentModelLoader.load_model(
            config.student_model_type,
            config.student_model_size,
            config.num_classes,
            self.device
        )

        # ç‰¹å¾å¯¹é½å±‚ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self.feature_aligner = None
        if config.align_feature and config.distillation_type in ['feature', 'hybrid']:
            # TODO: æ ¹æ®å®é™…æ¨¡å‹ç¡®å®šç‰¹å¾ç»´åº¦
            teacher_dim = 1280  # Qwen2.5-VLè§†è§‰ç¼–ç å™¨è¾“å‡ºç»´åº¦ï¼ˆéœ€æ ¹æ®å®é™…è°ƒæ•´ï¼‰
            student_dim = self._get_student_feature_dim()
            self.feature_aligner = FeatureAlignmentLayer(
                teacher_dim, student_dim, use_attention=True
            ).to(self.device)

        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self._setup_optimizer()
        self._setup_scheduler()

        # æŸå¤±å‡½æ•°
        self.ce_loss = nn.CrossEntropyLoss()
        self.mse_loss = nn.MSELoss()
        self.cosine_loss = nn.CosineEmbeddingLoss()

        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = torch.cuda.amp.GradScaler() if config.use_amp else None

        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.global_step = 0
        self.best_acc = 0.0

    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
        if torch.cuda.is_available():
            device_id = self.config.gpu_devices[0]
            device = torch.device(f"cuda:{device_id}")
            print(f"âœ“ ä½¿ç”¨GPUè®¾å¤‡: cuda:{device_id}")
        else:
            device = torch.device("cpu")
            print("âš ï¸  ä½¿ç”¨CPUè®­ç»ƒ")
        return device

    def _get_student_feature_dim(self) -> int:
        """è·å–å­¦ç”Ÿæ¨¡å‹ç‰¹å¾ç»´åº¦"""
        model_type = self.config.student_model_type
        size = self.config.student_model_size

        # æ ¹æ®æ¨¡å‹ç±»å‹è¿”å›ç‰¹å¾ç»´åº¦
        if model_type == 'resnet':
            dims = {'resnet18': 512, 'resnet34': 512, 'resnet50': 2048, 'resnet101': 2048}
            return dims.get(size, 2048)
        elif model_type == 'vit':
            dims = {'vit-tiny': 192, 'vit-base': 768, 'vit-large': 1024}
            return dims.get(size, 768)
        elif model_type == 'lstm':
            dims = {'small': 512, 'medium': 1024, 'large': 2048}
            return dims.get(size, 1024)
        else:
            return 512  # é»˜è®¤å€¼

    def _setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        params = list(self.student_model.parameters())
        if self.feature_aligner is not None:
            params += list(self.feature_aligner.parameters())

        if self.config.optimizer_type == 'adamw':
            self.optimizer = torch.optim.AdamW(
                params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer_type == 'adam':
            self.optimizer = torch.optim.Adam(
                params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer_type == 'sgd':
            self.optimizer = torch.optim.SGD(
                params,
                lr=self.config.learning_rate,
                momentum=0.9,
                weight_decay=self.config.weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.config.optimizer_type}")

    def _setup_scheduler(self):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # è®¡ç®—æ€»æ­¥æ•°
        # TODO: ä»æ•°æ®é›†è·å–å®é™…å¤§å°
        num_training_steps = (1000 // self.config.batch_size) * self.config.epochs

        if self.config.lr_scheduler == 'cosine':
            from torch.optim.lr_scheduler import CosineAnnealingLR
            self.scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=num_training_steps
            )
        elif self.config.lr_scheduler == 'linear':
            from torch.optim.lr_scheduler import LinearLR
            self.scheduler = LinearLR(
                self.optimizer,
                start_factor=1.0,
                end_factor=0.1,
                total_iters=num_training_steps
            )
        elif self.config.lr_scheduler == 'step':
            from torch.optim.lr_scheduler import StepLR
            self.scheduler = StepLR(
                self.optimizer,
                step_size=num_training_steps // 3,
                gamma=0.1
            )
        else:
            self.scheduler = None

    def extract_teacher_features(self, images: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        ä»Qwen2.5-VLæå–è§†è§‰ç‰¹å¾

        Returns:
            dict: {
                'vision_features': [B, N, D],  # è§†è§‰ç¼–ç å™¨è¾“å‡º
                'hidden_states': List[[B, N, D]],  # å„å±‚éšè—çŠ¶æ€
            }
        """
        if self.teacher_model is None:
            # æ¨¡æ‹Ÿæ¨¡å¼ï¼šè¿”å›éšæœºç‰¹å¾
            batch_size = images.size(0)
            return {
                'vision_features': torch.randn(batch_size, 256, 1280).to(self.device),
                'hidden_states': [torch.randn(batch_size, 256, 1280).to(self.device)]
            }

        with torch.no_grad():
            # Qwen2.5-VLçš„è§†è§‰ç¼–ç å™¨æå–ç‰¹å¾
            # æ³¨æ„ï¼šå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®Qwen2.5-VLçš„APIè°ƒæ•´
            outputs = self.teacher_model.visual(
                images,
                output_hidden_states=True
            )

            return {
                'vision_features': outputs.last_hidden_state,
                'hidden_states': outputs.hidden_states
            }

    def compute_distillation_loss(
        self,
        student_output: Any,
        teacher_features: Dict[str, torch.Tensor],
        labels: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—è’¸é¦æŸå¤±

        Args:
            student_output: å­¦ç”Ÿæ¨¡å‹è¾“å‡º
            teacher_features: æ•™å¸ˆæ¨¡å‹ç‰¹å¾
            labels: çœŸå®æ ‡ç­¾

        Returns:
            dict: {'total_loss': Tensor, 'hard_loss': Tensor, 'soft_loss': Tensor, ...}
        """
        losses = {}

        # 1. ç¡¬æ ‡ç­¾æŸå¤±ï¼ˆä»»åŠ¡æŸå¤±ï¼‰
        if self.config.task_type == 'classification':
            if isinstance(student_output, dict):
                logits = student_output['logits']
            else:
                logits = student_output
            hard_loss = self.ce_loss(logits, labels)
            losses['hard_loss'] = hard_loss
        else:
            # TODO: å®ç°æ£€æµ‹/åˆ†å‰²çš„ä»»åŠ¡æŸå¤±
            hard_loss = torch.tensor(0.0).to(self.device)
            losses['hard_loss'] = hard_loss

        # 2. è½¯æ ‡ç­¾æŸå¤±ï¼ˆLogitsè’¸é¦ï¼‰
        if self.config.distillation_type in ['logit', 'hybrid']:
            # TODO: ä»teacher_featuresæå–è½¯æ ‡ç­¾
            # è¿™éœ€è¦Qwen2.5-VLè¾“å‡ºåˆ†ç±»logits
            soft_loss = torch.tensor(0.0).to(self.device)
            losses['soft_loss'] = soft_loss

        # 3. ç‰¹å¾è’¸é¦æŸå¤±
        if self.config.distillation_type in ['feature', 'hybrid']:
            # æå–å­¦ç”Ÿæ¨¡å‹ç‰¹å¾
            student_features = self._extract_student_features(student_output)

            # å¯¹é½æ•™å¸ˆç‰¹å¾
            teacher_vis_features = teacher_features['vision_features']  # [B, N, D]

            if self.feature_aligner is not None:
                aligned_teacher_features = self.feature_aligner(
                    teacher_vis_features,
                    student_features
                )
            else:
                aligned_teacher_features = teacher_vis_features

            # è®¡ç®—ç‰¹å¾æŸå¤±
            if self.config.feature_loss_type == 'mse':
                # éœ€è¦ç¡®ä¿ç»´åº¦åŒ¹é…
                if student_features.shape != aligned_teacher_features.shape:
                    # ä½¿ç”¨æ± åŒ–æˆ–æ’å€¼è°ƒæ•´ç»´åº¦
                    student_features = F.adaptive_avg_pool1d(
                        student_features.transpose(1, 2),
                        aligned_teacher_features.size(1)
                    ).transpose(1, 2)

                feature_loss = self.mse_loss(student_features, aligned_teacher_features)

            elif self.config.feature_loss_type == 'cosine':
                # ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±
                student_norm = F.normalize(student_features.mean(dim=1), dim=-1)
                teacher_norm = F.normalize(aligned_teacher_features.mean(dim=1), dim=-1)
                target = torch.ones(student_norm.size(0)).to(self.device)
                feature_loss = self.cosine_loss(student_norm, teacher_norm, target)

            else:
                feature_loss = torch.tensor(0.0).to(self.device)

            losses['feature_loss'] = feature_loss

        # 4. æ€»æŸå¤±
        total_loss = (
            self.config.alpha * losses.get('hard_loss', 0) +
            self.config.beta * losses.get('soft_loss', 0) +
            self.config.gamma * losses.get('feature_loss', 0)
        )

        losses['total_loss'] = total_loss
        return losses

    def _extract_student_features(self, student_output) -> torch.Tensor:
        """ä»å­¦ç”Ÿæ¨¡å‹è¾“å‡ºä¸­æå–ç‰¹å¾"""
        # æ ¹æ®å­¦ç”Ÿæ¨¡å‹ç±»å‹æå–ç‰¹å¾
        if isinstance(student_output, dict):
            if 'hidden_states' in student_output:
                # ViTç­‰Transformeræ¨¡å‹
                return student_output['hidden_states'][-1]  # æœ€åä¸€å±‚
            elif 'last_hidden_state' in student_output:
                return student_output['last_hidden_state']

        # é»˜è®¤ï¼šå‡è®¾student_outputæ˜¯ç‰¹å¾å¼ é‡
        if len(student_output.shape) == 3:
            return student_output
        elif len(student_output.shape) == 2:
            return student_output.unsqueeze(1)
        else:
            return student_output.flatten(start_dim=1).unsqueeze(1)

    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.student_model.train()
        if self.feature_aligner is not None:
            self.feature_aligner.train()

        epoch_losses = {
            'total_loss': 0.0,
            'hard_loss': 0.0,
            'soft_loss': 0.0,
            'feature_loss': 0.0
        }

        num_batches = 0
        pbar = tqdm(train_loader, desc=f"Epoch {self.current_epoch + 1}/{self.config.epochs}")

        for batch_idx, batch in enumerate(pbar):
            images = batch['pixel_values'].to(self.device)
            labels = batch['labels'].to(self.device)

            # 1. æå–æ•™å¸ˆç‰¹å¾
            teacher_features = self.extract_teacher_features(images)

            # 2. å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
            if self.config.use_amp:
                with torch.cuda.amp.autocast():
                    student_output = self.student_model(images)
                    losses = self.compute_distillation_loss(
                        student_output, teacher_features, labels
                    )
                    loss = losses['total_loss'] / self.config.grad_accum_steps
            else:
                student_output = self.student_model(images)
                losses = self.compute_distillation_loss(
                    student_output, teacher_features, labels
                )
                loss = losses['total_loss'] / self.config.grad_accum_steps

            # 3. åå‘ä¼ æ’­
            if self.config.use_amp:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()

            # 4. æ¢¯åº¦ç´¯ç§¯
            if (batch_idx + 1) % self.config.grad_accum_steps == 0:
                if self.config.use_amp:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.student_model.parameters(),
                        self.config.max_grad_norm
                    )
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(
                        self.student_model.parameters(),
                        self.config.max_grad_norm
                    )
                    self.optimizer.step()

                if self.scheduler is not None:
                    self.scheduler.step()

                self.optimizer.zero_grad()
                self.global_step += 1

            # 5. è®°å½•æŸå¤±
            for key in epoch_losses:
                if key in losses:
                    epoch_losses[key] += losses[key].item()
            num_batches += 1

            # 6. æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{losses['total_loss'].item():.4f}",
                'lr': f"{self.optimizer.param_groups[0]['lr']:.6f}"
            })

            # 7. å®šæœŸæ—¥å¿—
            if (batch_idx + 1) % self.config.log_interval == 0:
                self._log_training_step(losses)

        # è®¡ç®—å¹³å‡æŸå¤±
        for key in epoch_losses:
            epoch_losses[key] /= num_batches

        return epoch_losses

    @torch.no_grad()
    def evaluate(self, val_loader: DataLoader) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.student_model.eval()

        total_loss = 0.0
        correct = 0
        total = 0

        for batch in tqdm(val_loader, desc="Evaluating"):
            images = batch['pixel_values'].to(self.device)
            labels = batch['labels'].to(self.device)

            # å‰å‘ä¼ æ’­
            outputs = self.student_model(images)

            # è®¡ç®—æŸå¤±
            if isinstance(outputs, dict):
                logits = outputs['logits']
            else:
                logits = outputs

            loss = self.ce_loss(logits, labels)
            total_loss += loss.item()

            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(logits, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        avg_loss = total_loss / len(val_loader)
        accuracy = 100.0 * correct / total

        return {
            'val_loss': avg_loss,
            'val_accuracy': accuracy
        }

    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\n{'='*60}")
        print("ğŸš€ å¼€å§‹è®­ç»ƒ")
        print(f"{'='*60}")
        print(f"æ•™å¸ˆæ¨¡å‹: Qwen2.5-VL 8B")
        print(f"å­¦ç”Ÿæ¨¡å‹: {self.config.student_model_type}-{self.config.student_model_size}")
        print(f"ä»»åŠ¡ç±»å‹: {self.config.task_type}")
        print(f"è’¸é¦ç­–ç•¥: {self.config.distillation_type}")
        print(f"è®­ç»ƒè½®æ•°: {self.config.epochs}")
        print(f"æ‰¹å¤§å°: {self.config.batch_size}")
        print(f"å­¦ä¹ ç‡: {self.config.learning_rate}")
        print(f"{'='*60}\n")

        for epoch in range(self.config.epochs):
            self.current_epoch = epoch

            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader)

            # è¯„ä¼°
            val_metrics = self.evaluate(val_loader)

            # æ‰“å°ç»“æœ
            print(f"\nEpoch {epoch + 1}/{self.config.epochs} å®Œæˆ:")
            print(f"  è®­ç»ƒæŸå¤±: {train_metrics['total_loss']:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_metrics['val_loss']:.4f}")
            print(f"  éªŒè¯å‡†ç¡®ç‡: {val_metrics['val_accuracy']:.2f}%")

            # å›è°ƒåç«¯APIæ›´æ–°è¿›åº¦
            self._update_training_progress(epoch + 1, train_metrics, val_metrics)

            # ä¿å­˜checkpoint
            if (epoch + 1) % self.config.checkpoint_interval == 0:
                self.save_checkpoint(epoch + 1, val_metrics['val_accuracy'])

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['val_accuracy'] > self.best_acc:
                self.best_acc = val_metrics['val_accuracy']
                self.save_checkpoint(epoch + 1, val_metrics['val_accuracy'], is_best=True)

        print(f"\n{'='*60}")
        print("âœ“ è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_acc:.2f}%")
        print(f"{'='*60}\n")

    def save_checkpoint(self, epoch: int, accuracy: float, is_best: bool = False):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.student_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'accuracy': accuracy,
            'config': vars(self.config)
        }

        if self.feature_aligner is not None:
            checkpoint['aligner_state_dict'] = self.feature_aligner.state_dict()

        # ä¿å­˜è·¯å¾„
        os.makedirs(self.config.output_dir, exist_ok=True)

        if is_best:
            path = os.path.join(self.config.output_dir, 'best_model.pt')
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {path}")
        else:
            path = os.path.join(self.config.output_dir, f'checkpoint_epoch_{epoch}.pt')
            print(f"ğŸ’¾ ä¿å­˜checkpoint: {path}")

        torch.save(checkpoint, path)

    def _update_training_progress(self, epoch: int, train_metrics: Dict, val_metrics: Dict):
        """å›è°ƒåç«¯APIæ›´æ–°è®­ç»ƒè¿›åº¦"""
        try:
            url = f"{self.config.api_base_url}/training/progress/{self.config.task_id}"
            data = {
                'epoch': epoch,
                'total_epochs': self.config.epochs,
                'train_loss': train_metrics['total_loss'],
                'val_loss': val_metrics['val_loss'],
                'val_accuracy': val_metrics['val_accuracy'],
                'timestamp': datetime.now().isoformat()
            }

            response = requests.post(url, json=data, timeout=5)
            if response.status_code != 200:
                print(f"âš ï¸  è¿›åº¦æ›´æ–°å¤±è´¥: {response.text}")
        except Exception as e:
            print(f"âš ï¸  è¿›åº¦æ›´æ–°å¼‚å¸¸: {e}")

    def _log_training_step(self, losses: Dict[str, torch.Tensor]):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        # è¿™é‡Œå¯ä»¥é›†æˆTensorBoardæˆ–å…¶ä»–æ—¥å¿—å·¥å…·
        pass


# ==================== å‘½ä»¤è¡Œå‚æ•°è§£æ ====================

def parse_args():
    parser = argparse.ArgumentParser(description='Qwen2.5-VLå¤šæ¨¡å‹ååŒè®­ç»ƒ')

    # åŸºç¡€é…ç½®
    parser.add_argument('--task_id', type=str, required=True, help='ä»»åŠ¡ID')
    parser.add_argument('--api_base_url', type=str, required=True, help='åç«¯APIåœ°å€')

    # æ•™å¸ˆæ¨¡å‹é…ç½®
    parser.add_argument('--teacher_model_path', type=str, required=True,
                       help='Qwen2.5-VLæ¨¡å‹è·¯å¾„')

    # å­¦ç”Ÿæ¨¡å‹é…ç½®
    parser.add_argument('--student_model_type', type=str, required=True,
                       choices=['resnet', 'vit', 'yolov8', 'unet', 'lstm'],
                       help='å­¦ç”Ÿæ¨¡å‹ç±»å‹')
    parser.add_argument('--student_model_size', type=str, required=True,
                       help='å­¦ç”Ÿæ¨¡å‹å¤§å° (å¦‚resnet50, vit-baseç­‰)')
    parser.add_argument('--student_model_path', type=str, default=None,
                       help='å­¦ç”Ÿæ¨¡å‹é¢„è®­ç»ƒæƒé‡è·¯å¾„')

    # ä»»åŠ¡é…ç½®
    parser.add_argument('--task_type', type=str, default='classification',
                       choices=['classification', 'detection', 'segmentation'],
                       help='ä»»åŠ¡ç±»å‹')
    parser.add_argument('--num_classes', type=int, default=10, help='ç±»åˆ«æ•°')

    # æ•°æ®é…ç½®
    parser.add_argument('--dataset_path', type=str, required=True, help='è®­ç»ƒæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--val_dataset_path', type=str, required=True, help='éªŒè¯æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--image_size', type=int, default=224, help='å›¾åƒå¤§å°')

    # è®­ç»ƒè¶…å‚æ•°
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=32, help='æ‰¹å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--optimizer_type', type=str, default='adamw',
                       choices=['adamw', 'adam', 'sgd'])
    parser.add_argument('--lr_scheduler', type=str, default='cosine',
                       choices=['cosine', 'linear', 'step'])
    parser.add_argument('--weight_decay', type=float, default=0.01, help='æƒé‡è¡°å‡')
    parser.add_argument('--grad_accum_steps', type=int, default=1, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    parser.add_argument('--max_grad_norm', type=float, default=1.0, help='æ¢¯åº¦è£å‰ª')

    # è’¸é¦é…ç½®
    parser.add_argument('--distillation_type', type=str, default='hybrid',
                       choices=['feature', 'logit', 'layer', 'hybrid'],
                       help='è’¸é¦ç±»å‹')
    parser.add_argument('--temperature', type=float, default=4.0, help='è’¸é¦æ¸©åº¦')
    parser.add_argument('--alpha', type=float, default=0.5, help='ç¡¬æ ‡ç­¾æƒé‡')
    parser.add_argument('--beta', type=float, default=0.3, help='è½¯æ ‡ç­¾æƒé‡')
    parser.add_argument('--gamma', type=float, default=0.2, help='ç‰¹å¾è’¸é¦æƒé‡')
    parser.add_argument('--feature_loss_type', type=str, default='mse',
                       choices=['mse', 'cosine', 'attention'])
    parser.add_argument('--align_feature', action='store_true', help='ä½¿ç”¨ç‰¹å¾å¯¹é½å±‚')
    parser.add_argument('--feature_dim', type=int, default=768, help='ç‰¹å¾å¯¹é½ç»´åº¦')

    # GPUé…ç½®
    parser.add_argument('--gpu_devices', type=str, default='0', help='GPUè®¾å¤‡IDï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--use_amp', action='store_true', help='ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')

    # è¾“å‡ºé…ç½®
    parser.add_argument('--output_dir', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--checkpoint_interval', type=int, default=10, help='ä¿å­˜checkpointé—´éš”')
    parser.add_argument('--log_interval', type=int, default=50, help='æ—¥å¿—æ‰“å°é—´éš”')

    return parser.parse_args()


# ==================== ä¸»å‡½æ•° ====================

def main():
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     Qwen2.5-VL å¤šæ¨¡å‹ååŒè®­ç»ƒç³»ç»Ÿ                            â•‘
    â•‘     Multi-Model Collaborative Training with Qwen2.5-VL       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # è§£æå‚æ•°
    args = parse_args()
    config = TrainingConfig(args)

    # åˆ›å»ºæ•°æ®é›†
    print("\næ­£åœ¨åŠ è½½æ•°æ®é›†...")
    train_dataset = MultiTaskDataset(
        config.dataset_path,
        config.task_type,
        config.image_size,
        config.num_classes,
        mode='train'
    )
    val_dataset = MultiTaskDataset(
        config.val_dataset_path,
        config.task_type,
        config.image_size,
        config.num_classes,
        mode='val'
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )

    print(f"âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"âœ“ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MultiModelDistillationTrainer(config)

    # å¼€å§‹è®­ç»ƒ
    trainer.train(train_loader, val_loader)

    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")


if __name__ == '__main__':
    main()
