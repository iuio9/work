#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen2.5-VLåˆ°å¤šæ¶æ„å°æ¨¡å‹çš„çŸ¥è¯†è’¸é¦è®­ç»ƒè„šæœ¬

æ”¯æŒçš„æ•™å¸ˆæ¨¡å‹ï¼š
- Qwen2.5-VL 3Bï¼ˆå¤šæ¨¡æ€è§†è§‰-è¯­è¨€æ¨¡å‹ï¼‰

æ”¯æŒçš„å­¦ç”Ÿæ¨¡å‹ï¼š
- LSTMï¼šåºåˆ—ç‰¹å¾æå– + å›¾åƒåˆ†ç±»
- UNetï¼šå›¾åƒåˆ†å‰²
- YOLOv8ï¼šç›®æ ‡æ£€æµ‹
- ResNetï¼šå›¾åƒåˆ†ç±»
- Vision Transformerï¼šå›¾åƒåˆ†ç±»

ä½œè€…ï¼šClaude Assistant
æ—¥æœŸï¼š2026-01-11
ç‰ˆæœ¬ï¼š1.0.0
"""

import argparse
import json
import os
import sys
import time
import warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

import numpy as np
import requests
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
from tqdm import tqdm

# Qwen2.5-VLç›¸å…³å¯¼å…¥
try:
    from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
    QWEN_AVAILABLE = True
except ImportError:
    QWEN_AVAILABLE = False
    warnings.warn("Qwen2_5_VLæ¨¡å‹åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

# å°æ¨¡å‹ç›¸å…³å¯¼å…¥
import torchvision.models as models
from transformers import (
    AutoConfig,
    AutoModelForImageClassification,
    AutoImageProcessor,
    ViTForImageClassification,
    ViTImageProcessor
)
from peft import LoraConfig, get_peft_model, TaskType
from qwen_vl_utils import process_vision_info

# YOLOç›¸å…³
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    warnings.warn("YOLOv8æœªå®‰è£…ï¼Œä½¿ç”¨: pip install ultralytics")


# ==================== é…ç½®ç±» ====================

class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»ï¼Œä¸åç«¯TrainingExecutionServiceä¿æŒä¸€è‡´"""

    def __init__(self, args):
        # åŸºç¡€é…ç½®
        self.task_id = args.task_id
        self.api_base_url = args.api_base_url

        # æ¨¡å‹é…ç½®
        self.teacher_model = args.teacher_model  # "qwen2.5-vl-8b"
        self.student_model = args.student_model  # "resnet50", "vit-base", etc.
        self.teacher_path = args.teacher_path
        self.student_path = args.student_path

        # å­¦ç”Ÿæ¨¡å‹ç±»å‹å’Œå¤§å°
        self.student_model_type = args.student_model_type  # resnet/vit/yolov8/unet/lstm
        self.student_model_size = args.student_model_size  # resnet50, vit-base, s, medium, etc.

        # ä»»åŠ¡é…ç½®
        self.task_type = args.task_type  # classification/detection/segmentation
        self.num_classes = args.num_classes

        # æ•°æ®é…ç½®
        self.dataset_id = args.dataset_id
        self.val_dataset_id = args.val_dataset_id
        self.datasets_root = args.datasets_root
        self.image_size = args.image_size

        # è®­ç»ƒå‚æ•°
        self.epochs = args.epochs
        self.batch_size = args.batch_size
        self.learning_rate = args.learning_rate

        # ä¼˜åŒ–å™¨é…ç½®
        self.optimizer = args.optimizer
        self.lr_scheduler = args.lr_scheduler
        self.weight_decay = args.weight_decay
        self.grad_accum_steps = args.grad_accum_steps
        self.max_grad_norm = args.max_grad_norm

        # GPUé…ç½®
        self.gpu_devices = self._parse_gpu_devices(args.gpu_devices)
        self.auto_save_checkpoint = args.auto_save_checkpoint
        self.checkpoint_interval = args.checkpoint_interval

        # LoRAé…ç½®
        self.lora_rank = args.lora_rank
        self.lora_alpha = args.lora_alpha
        self.lora_dropout = args.lora_dropout
        self.lora_target_modules = self._parse_list(args.lora_target_modules)
        self.lora_bias = args.lora_bias

        # çŸ¥è¯†è’¸é¦é…ç½®
        self.temperature = args.temperature
        self.hard_label_weight = args.hard_label_weight
        self.soft_label_weight = args.soft_label_weight
        self.distill_loss_type = args.distill_loss_type

        # è’¸é¦ç­–ç•¥
        self.distillation_type = args.distillation_type  # feature/logit/hybrid
        self.feature_loss_type = args.feature_loss_type  # mse/cosine
        self.align_feature = args.align_feature

        # è¾“å‡ºé…ç½®
        self.output_dir = args.output_dir

    def _parse_gpu_devices(self, gpu_str: str) -> List[int]:
        if not gpu_str or gpu_str == "":
            return [0]
        return [int(x.strip()) for x in gpu_str.split(",")]

    def _parse_list(self, list_str: str) -> List[str]:
        if not list_str or list_str == "":
            return []
        return [x.strip() for x in list_str.split(",")]


# ==================== æ•°æ®é›†ç±» ====================

class MultiTaskDataset(Dataset):
    """
    å¤šä»»åŠ¡æ•°æ®é›†ï¼Œæ”¯æŒåˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²

    æœŸæœ›çš„ç›®å½•ç»“æ„ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰ï¼š
    dataset_path/
      â”œâ”€â”€ class1/
      â”‚   â”œâ”€â”€ img1.jpg
      â”‚   â””â”€â”€ img2.jpg
      â”œâ”€â”€ class2/
      â”‚   â””â”€â”€ ...
    """

    def __init__(
        self,
        dataset_path: str,
        task_type: str = 'classification',
        image_size: int = 224,
        num_classes: int = 10,
        mode: str = 'train'
    ):
        self.dataset_path = dataset_path
        self.task_type = task_type
        self.image_size = image_size
        self.num_classes = num_classes
        self.mode = mode

        # å­˜å‚¨æ‰€æœ‰å›¾åƒè·¯å¾„å’Œæ ‡ç­¾
        self.image_paths = []
        self.labels = []
        self.class_names = []

        # æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦å­˜åœ¨
        if os.path.exists(dataset_path):
            self._load_dataset()
        else:
            print(f"âš ï¸ è­¦å‘Š: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
            print(f"ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º")
            self._use_mock_data(1000 if mode == 'train' else 200)

        # æ•°æ®å¢å¼º
        if mode == 'train':
            self.transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.RandomHorizontalFlip(),
                transforms.RandomRotation(15),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
        else:
            self.transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])

    def _load_dataset(self):
        """ä»ç›®å½•ç»“æ„åŠ è½½çœŸå®æ•°æ®é›†ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰"""
        print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {self.dataset_path}")

        # è·å–æ‰€æœ‰ç±»åˆ«æ–‡ä»¶å¤¹
        class_folders = sorted([d for d in os.listdir(self.dataset_path)
                               if os.path.isdir(os.path.join(self.dataset_path, d))])

        if not class_folders:
            print(f"âš ï¸ è­¦å‘Š: åœ¨ {self.dataset_path} ä¸­æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶å¤¹")
            self._use_mock_data(1000 if self.mode == 'train' else 200)
            return

        self.class_names = class_folders
        print(f"âœ… æ‰¾åˆ° {len(self.class_names)} ä¸ªç±»åˆ«: {self.class_names}")

        # éå†æ¯ä¸ªç±»åˆ«æ–‡ä»¶å¤¹
        for class_idx, class_name in enumerate(self.class_names):
            class_dir = os.path.join(self.dataset_path, class_name)

            # è·å–è¯¥ç±»åˆ«ä¸‹çš„æ‰€æœ‰å›¾åƒæ–‡ä»¶
            image_files = [f for f in os.listdir(class_dir)
                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]

            for img_file in image_files:
                img_path = os.path.join(class_dir, img_file)
                self.image_paths.append(img_path)
                self.labels.append(class_idx)

        print(f"âœ… åŠ è½½å®Œæˆ: å…± {len(self.image_paths)} å¼ å›¾åƒ")

        # æ›´æ–°ç±»åˆ«æ•°
        if self.num_classes is None or self.num_classes != len(self.class_names):
            self.num_classes = len(self.class_names)

    def _use_mock_data(self, num_samples):
        """ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆå½“çœŸå®æ•°æ®ä¸å¯ç”¨æ—¶ï¼‰"""
        print(f"ğŸ­ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {num_samples} ä¸ªæ ·æœ¬")
        self.class_names = [f"class_{i}" for i in range(self.num_classes)]

        # ç”Ÿæˆæ¨¡æ‹Ÿè·¯å¾„å’Œæ ‡ç­¾
        for i in range(num_samples):
            self.image_paths.append(f"mock_image_{i}.jpg")
            self.labels.append(i % len(self.class_names))

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]

        # å°è¯•åŠ è½½çœŸå®å›¾åƒ
        try:
            if os.path.exists(img_path):
                image = Image.open(img_path).convert('RGB')
            else:
                # ç”Ÿæˆéšæœºå›¾åƒï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
                image_array = np.random.randint(0, 255, (self.image_size, self.image_size, 3), dtype=np.uint8)
                image = Image.fromarray(image_array)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å›¾åƒå¤±è´¥ {img_path}: {e}")
            # ç”Ÿæˆéšæœºå›¾åƒä½œä¸ºåå¤‡
            image_array = np.random.randint(0, 255, (self.image_size, self.image_size, 3), dtype=np.uint8)
            image = Image.fromarray(image_array)

        # åº”ç”¨å˜æ¢
        pixel_values = self.transform(image)

        if self.task_type == 'classification':
            return {'pixel_values': pixel_values, 'labels': label}
        elif self.task_type == 'detection':
            # æ£€æµ‹ä»»åŠ¡çš„æ¨¡æ‹Ÿæ•°æ®
            num_boxes = np.random.randint(1, 5)
            boxes = torch.rand(num_boxes, 4)
            box_labels = torch.randint(0, self.num_classes, (num_boxes,))
            return {'pixel_values': pixel_values, 'boxes': boxes, 'labels': box_labels}
        elif self.task_type == 'segmentation':
            # åˆ†å‰²ä»»åŠ¡çš„æ¨¡æ‹Ÿæ•°æ®
            mask = torch.randint(0, self.num_classes, (self.image_size, self.image_size))
            return {'pixel_values': pixel_values, 'mask': mask}


# ==================== æ¨¡å‹åŠ è½½å™¨ ====================

class TeacherModelLoader:
    """Qwen2.5-VLæ•™å¸ˆæ¨¡å‹åŠ è½½å™¨"""

    @staticmethod
    def load_qwen2vl(model_path: str, device: torch.device):
        if not QWEN_AVAILABLE:
            print("âš ï¸  Qwen2VLæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹")
            return None, None

        print(f"æ­£åœ¨åŠ è½½Qwen2.5-VLæ•™å¸ˆæ¨¡å‹: {model_path}")

        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_path,
            torch_dtype="auto",
            device_map="cpu"
        )
        processor = AutoProcessor.from_pretrained(model_path)

        model.eval()
        for param in model.parameters():
            param.requires_grad = False

        print(f"âœ“ Qwen2.5-VLåŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        return model, processor


class StudentModelLoader:
    """å­¦ç”Ÿæ¨¡å‹åŠ è½½å™¨"""

    @staticmethod
    def load_model(
        model_type: str,
        model_size: str,
        num_classes: int,
        device: torch.device,
        pretrained: bool = True
    ):
        print(f"\næ­£åœ¨åŠ è½½å­¦ç”Ÿæ¨¡å‹: {model_type}-{model_size}")

        if model_type == 'resnet':
            return StudentModelLoader._load_resnet(model_size, num_classes, device, pretrained)
        elif model_type == 'vit':
            return StudentModelLoader._load_vit(model_size, num_classes, device, pretrained)
        elif model_type == 'yolov8':
            return StudentModelLoader._load_yolov8(model_size, num_classes, device)
        elif model_type == 'unet':
            return StudentModelLoader._load_unet(model_size, num_classes, device)
        elif model_type == 'lstm':
            return StudentModelLoader._load_lstm(model_size, num_classes, device)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å­¦ç”Ÿæ¨¡å‹ç±»å‹: {model_type}")

    @staticmethod
    def _load_resnet(size: str, num_classes: int, device, pretrained: bool):
        resnet_variants = {
            'resnet18': models.resnet18,
            'resnet34': models.resnet34,
            'resnet50': models.resnet50,
            'resnet101': models.resnet101,
        }

        if size not in resnet_variants:
            raise ValueError(f"ä¸æ”¯æŒçš„ResNetå˜ä½“: {size}")

        model = resnet_variants[size](pretrained=pretrained)
        in_features = model.fc.in_features
        model.fc = nn.Linear(in_features, num_classes)
        model.to(device)

        print(f"âœ“ ResNet-{size}åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        return model

    @staticmethod
    def _load_vit(size: str, num_classes: int, device, pretrained: bool):
        vit_models = {
            'vit-base': 'google/vit-base-patch16-224',
            'vit-large': 'google/vit-large-patch16-224',
            'vit-tiny': 'WinKawaks/vit-tiny-patch16-224',
        }

        if size not in vit_models:
            raise ValueError(f"ä¸æ”¯æŒçš„ViTå˜ä½“: {size}")

        model_name = vit_models[size]
        model = ViTForImageClassification.from_pretrained(
            model_name,
            num_labels=num_classes,
            ignore_mismatched_sizes=True
        )
        model.to(device)

        print(f"âœ“ ViT-{size}åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        return model

    @staticmethod
    def _load_yolov8(size: str, num_classes: int, device):
        if not YOLO_AVAILABLE:
            raise ImportError("YOLOv8æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install ultralytics")

        yolo_sizes = {'n': 'yolov8n.pt', 's': 'yolov8s.pt', 'm': 'yolov8m.pt',
                     'l': 'yolov8l.pt', 'x': 'yolov8x.pt'}

        if size not in yolo_sizes:
            raise ValueError(f"ä¸æ”¯æŒçš„YOLOå¤§å°: {size}")

        model = YOLO(yolo_sizes[size])
        print(f"âœ“ YOLOv8-{size}åŠ è½½æˆåŠŸ")
        return model

    @staticmethod
    def _load_unet(size: str, num_classes: int, device):
        class SimpleUNet(nn.Module):
            def __init__(self, in_channels=3, num_classes=10):
                super().__init__()
                self.enc1 = self._conv_block(in_channels, 64)
                self.enc2 = self._conv_block(64, 128)
                self.enc3 = self._conv_block(128, 256)
                self.enc4 = self._conv_block(256, 512)

                self.dec3 = self._conv_block(512 + 256, 256)
                self.dec2 = self._conv_block(256 + 128, 128)
                self.dec1 = self._conv_block(128 + 64, 64)

                self.final = nn.Conv2d(64, num_classes, 1)
                self.pool = nn.MaxPool2d(2)
                self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

            def _conv_block(self, in_ch, out_ch):
                return nn.Sequential(
                    nn.Conv2d(in_ch, out_ch, 3, padding=1),
                    nn.BatchNorm2d(out_ch),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(out_ch, out_ch, 3, padding=1),
                    nn.BatchNorm2d(out_ch),
                    nn.ReLU(inplace=True)
                )

            def forward(self, x):
                e1 = self.enc1(x)
                e2 = self.enc2(self.pool(e1))
                e3 = self.enc3(self.pool(e2))
                e4 = self.enc4(self.pool(e3))

                d3 = self.dec3(torch.cat([self.upsample(e4), e3], dim=1))
                d2 = self.dec2(torch.cat([self.upsample(d3), e2], dim=1))
                d1 = self.dec1(torch.cat([self.upsample(d2), e1], dim=1))

                return self.final(d1)

        model = SimpleUNet(in_channels=3, num_classes=num_classes)
        model.to(device)
        print(f"âœ“ UNetåŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        return model

    @staticmethod
    def _load_lstm(size: str, num_classes: int, device):
        class LSTMClassifier(nn.Module):
            def __init__(self, input_size=2048, hidden_size=512, num_layers=2, num_classes=10):
                super().__init__()
                self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                                   batch_first=True, bidirectional=True)
                self.fc = nn.Linear(hidden_size * 2, num_classes)
                self.dropout = nn.Dropout(0.5)

                resnet = models.resnet50(pretrained=True)
                self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])

            def forward(self, x):
                batch_size = x.size(0)
                features = self.feature_extractor(x)
                features = features.view(batch_size, -1, 1)
                features = features.transpose(1, 2)

                lstm_out, _ = self.lstm(features)
                lstm_out = lstm_out[:, -1, :]
                lstm_out = self.dropout(lstm_out)

                output = self.fc(lstm_out)
                return output

        hidden_sizes = {'small': 256, 'medium': 512, 'large': 1024}
        hidden_size = hidden_sizes.get(size, 512)

        model = LSTMClassifier(hidden_size=hidden_size, num_classes=num_classes)
        model.to(device)
        print(f"âœ“ LSTMåŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        return model


# ==================== ç‰¹å¾å¯¹é½å±‚ ====================

class FeatureAlignmentLayer(nn.Module):
    """ç‰¹å¾å¯¹é½å±‚ï¼šå°†æ•™å¸ˆæ¨¡å‹ç‰¹å¾æŠ•å½±åˆ°å­¦ç”Ÿæ¨¡å‹ç‰¹å¾ç©ºé—´"""

    def __init__(self, teacher_dim: int, student_dim: int, use_attention: bool = False):
        super().__init__()
        self.use_attention = use_attention

        self.projection = nn.Sequential(
            nn.Linear(teacher_dim, student_dim),
            nn.LayerNorm(student_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        if use_attention:
            self.attention = nn.MultiheadAttention(student_dim, num_heads=8, batch_first=True)

    def forward(self, teacher_features, student_features=None):
        aligned = self.projection(teacher_features)

        if self.use_attention and student_features is not None:
            aligned, _ = self.attention(aligned, student_features, student_features)

        return aligned


# ==================== çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨ ====================

class QwenMultiModelDistillationTrainer:
    """Qwen2.5-VLåˆ°å¤šç§å°æ¨¡å‹çš„çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨"""

    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = self._setup_device()

        # åŠ è½½æ•™å¸ˆæ¨¡å‹ï¼ˆQwen2.5-VLï¼‰
        self.teacher_model, self.teacher_processor = TeacherModelLoader.load_qwen2vl(
            config.teacher_path,
            self.device
        )

        # åŠ è½½å­¦ç”Ÿæ¨¡å‹
        self.student_model = StudentModelLoader.load_model(
            config.student_model_type,
            config.student_model_size,
            config.num_classes,
            self.device
        )
        self.student_feature_map = None
        if config.student_model_type == "resnet":
            def hook_fn(module, input, output):
                self.student_feature_map = output

            self.student_model.avgpool.register_forward_hook(hook_fn)
        self.feature_aligner = None
        if config.align_feature and config.distillation_type in ['feature', 'hybrid']:

            student_dim = self._get_student_feature_dim()

            if self.teacher_model is not None:
                teacher_dim = self.teacher_model.config.vision_config.hidden_size
            else:
                teacher_dim = 1024  # fallback

            self.feature_aligner = FeatureAlignmentLayer(
                teacher_dim,
                student_dim,
                use_attention=False
            ).to(self.device)
        # ç‰¹å¾å¯¹é½å±‚

#        '''' if config.align_feature and config.distillation_type in ['feature', 'hybrid']:
#             teacher_dim = 1280  # Qwen2.5-VLè§†è§‰ç¼–ç å™¨ç»´åº¦
#             student_dim = self._get_student_feature_dim()
#             self.feature_aligner = FeatureAlignmentLayer(
#                 teacher_dim, student_dim, use_attention=True
#             ).to(self.device)''''

        # åº”ç”¨LoRAï¼ˆå¦‚æœéœ€è¦ï¼‰
        if config.lora_rank > 0 and config.student_model_type == "vit":
            self._apply_lora_to_student()

        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self._setup_optimizer()
        self._setup_scheduler()

        # æŸå¤±å‡½æ•°
        self.ce_loss = nn.CrossEntropyLoss()
        self.mse_loss = nn.MSELoss()
        self.cosine_loss = nn.CosineEmbeddingLoss()

        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.global_step = 0
        self.best_acc = 0.0

    def _setup_device(self) -> torch.device:
#         if torch.cuda.is_available():
#             device_id = self.config.gpu_devices[0]
#             device = torch.device(f"cuda:{device_id}")
#             print(f"âœ“ ä½¿ç”¨GPUè®¾å¤‡: cuda:{device_id}")
#         else:
        device = torch.device("cpu")
        print("âš ï¸  ä½¿ç”¨CPUè®­ç»ƒ")
        return device
    def _get_student_feature_dim(self) -> int:
        model_type = self.config.student_model_type
        size = self.config.student_model_size

        if model_type == 'resnet':
            dims = {'resnet18': 512, 'resnet34': 512, 'resnet50': 2048, 'resnet101': 2048}
            return dims.get(size, 2048)
        elif model_type == 'vit':
            dims = {'vit-tiny': 192, 'vit-base': 768, 'vit-large': 1024}
            return dims.get(size, 768)
        elif model_type == 'lstm':
            dims = {'small': 512, 'medium': 1024, 'large': 2048}
            return dims.get(size, 1024)
        else:
            return 512

    def _apply_lora_to_student(self):
        """å¯¹å­¦ç”Ÿæ¨¡å‹åº”ç”¨LoRA"""
        if self.config.student_model_type == 'vit':
            task_type = TaskType.IMAGE_CLASSIFICATION
        else:
            task_type = TaskType.SEQ_CLS

        lora_config = LoraConfig(
            task_type=task_type,
            inference_mode=False,
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=self.config.lora_target_modules if self.config.lora_target_modules else None,
            bias=self.config.lora_bias
        )

        self.student_model = get_peft_model(self.student_model, lora_config)
        print(f"âœ“ LoRAå·²åº”ç”¨åˆ°å­¦ç”Ÿæ¨¡å‹ï¼Œå¯è®­ç»ƒå‚æ•°: {self.student_model.get_nb_trainable_parameters()}")

    def _setup_optimizer(self):
        params = list(self.student_model.parameters())
        if self.feature_aligner is not None:
            params += list(self.feature_aligner.parameters())

        if self.config.optimizer == 'adamw':
            self.optimizer = torch.optim.AdamW(
                params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer == 'adam':
            self.optimizer = torch.optim.Adam(
                params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer == 'sgd':
            self.optimizer = torch.optim.SGD(
                params,
                lr=self.config.learning_rate,
                momentum=0.9,
                weight_decay=self.config.weight_decay
            )

    def _setup_scheduler(self):
        num_training_steps = (1000 // self.config.batch_size) * self.config.epochs

        if self.config.lr_scheduler == 'cosine':
            from torch.optim.lr_scheduler import CosineAnnealingLR
            self.scheduler = CosineAnnealingLR(self.optimizer, T_max=num_training_steps)
        elif self.config.lr_scheduler == 'linear':
            from torch.optim.lr_scheduler import LinearLR
            self.scheduler = LinearLR(self.optimizer, start_factor=1.0,
                                     end_factor=0.1, total_iters=num_training_steps)
        else:
            self.scheduler = None
    def extract_teacher_features(self, images: torch.Tensor):
        """
        æå–æ•™å¸ˆæ¨¡å‹çš„è§†è§‰ç‰¹å¾
        """
        batch_size = images.size(0)

        # å¦‚æœæ²¡æœ‰æ•™å¸ˆæ¨¡å‹ï¼Œè¿”å›éšæœºç‰¹å¾
        if self.teacher_model is None:
            return {
                'vision_features': torch.randn(batch_size, 256, 1024).to(self.device)
            }

        # è½¬ PIL å›¾åƒ
        pil_images = [transforms.ToPILImage()(img.cpu()) for img in images]

        # è·å– processor è¾“å‡º
        inputs = self.teacher_processor(
            images=pil_images,
            text=["image"] * batch_size,
            return_tensors="pt",
        )

        # å•ç‹¬å¤„ç† tensor ç±»å‹å’Œ device
        for k, v in inputs.items():
            if k == "input_ids":  # embedding è¾“å…¥å¿…é¡»æ˜¯ LongTensor
                inputs[k] = v.long().to(self.device)
            else:
                inputs[k] = v.to(self.device)

        # æå–ç‰¹å¾
        with torch.no_grad():
            outputs = self.teacher_model(**inputs, output_hidden_states=True)

        # æ ¹æ®æ¨¡å‹è¾“å‡ºè·å–è§†è§‰ç‰¹å¾
        if hasattr(outputs, "vision_hidden_states"):
            vision_features = outputs.vision_hidden_states[-1]
        else:
            vision_features = outputs.last_hidden_state  # æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´

        return {
            'vision_features': vision_features
        }

#     '''def extract_teacher_features(self, images: torch.Tensor) -> Dict[str, torch.Tensor]:
#         """ä»Qwen2.5-VLæå–è§†è§‰ç‰¹å¾"""
#         if self.teacher_model is None:
#             # æ¨¡æ‹Ÿæ¨¡å¼
#             batch_size = images.size(0)
#             return {
#                 'vision_features': torch.randn(batch_size, 256, 1280).to(self.device),
#                 'hidden_states': [torch.randn(batch_size, 256, 1280).to(self.device)]
#             }
#
#         with torch.no_grad():
#             outputs = self.teacher_model.visual(images, output_hidden_states=True)
#             return {
#                 'vision_features': outputs.last_hidden_state,
#                 'hidden_states': outputs.hidden_states
#             }''''

    def compute_distillation_loss(
        self,
        student_output: Any,
        teacher_features: Dict[str, torch.Tensor],
        labels: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """è®¡ç®—è’¸é¦æŸå¤±"""
        losses = {}

        # ç¡¬æ ‡ç­¾æŸå¤±
        if self.config.task_type == 'classification':
            if isinstance(student_output, dict):
                logits = student_output['logits']
            else:
                logits = student_output
            hard_loss = self.ce_loss(logits, labels)
            losses['hard_loss'] = hard_loss
        else:
            hard_loss = torch.tensor(0.0).to(self.device)
            losses['hard_loss'] = hard_loss

        # è½¯æ ‡ç­¾æŸå¤±ï¼ˆæš‚æ—¶ç®€åŒ–ï¼‰
        soft_loss = torch.tensor(0.0).to(self.device)
        losses['soft_loss'] = soft_loss

        # ç‰¹å¾è’¸é¦æŸå¤±
        if self.config.distillation_type in ['feature', 'hybrid']:
            student_features = self._extract_student_features(student_output)
            teacher_vis_features = teacher_features['vision_features']

            if self.feature_aligner is not None:
                aligned_teacher_features = self.feature_aligner(
                    teacher_vis_features, student_features
                )
            else:
                aligned_teacher_features = teacher_vis_features

            if self.config.feature_loss_type == 'mse':
                if student_features.shape != aligned_teacher_features.shape:
                    student_features = F.adaptive_avg_pool1d(
                        student_features.transpose(1, 2),
                        aligned_teacher_features.size(1)
                    ).transpose(1, 2)
                feature_loss = self.mse_loss(student_features, aligned_teacher_features)
            elif self.config.feature_loss_type == 'cosine':
                student_norm = F.normalize(student_features.mean(dim=1), dim=-1)
                teacher_norm = F.normalize(aligned_teacher_features.mean(dim=1), dim=-1)
                target = torch.ones(student_norm.size(0)).to(self.device)
                feature_loss = self.cosine_loss(student_norm, teacher_norm, target)
            else:
                feature_loss = torch.tensor(0.0).to(self.device)

            losses['feature_loss'] = feature_loss

        # æ€»æŸå¤±
        total_loss = (
            self.config.hard_label_weight * losses.get('hard_loss', 0) +
            self.config.soft_label_weight * losses.get('soft_loss', 0)
        )

        if 'feature_loss' in losses:
            # ç‰¹å¾æŸå¤±æƒé‡å¯ä»¥é€šè¿‡è’¸é¦é…ç½®è°ƒæ•´
            feature_weight = 0.2
            total_loss += feature_weight * losses['feature_loss']

        losses['total_loss'] = total_loss
        return losses

    def _extract_student_features(self, student_output):

        if self.student_feature_map is not None:
            feat = self.student_feature_map  # [B, C, H, W]
            feat = feat.flatten(2).transpose(1, 2)
            return feat  # [B, HW, C]

        raise RuntimeError("âŒ æœªæ•è·å­¦ç”Ÿæ¨¡å‹ backbone ç‰¹å¾ï¼Œè¯·æ£€æŸ¥ hook")
#     '''def _extract_student_features(self, student_output) -> torch.Tensor:
#         if isinstance(student_output, dict):
#             if 'hidden_states' in student_output:
#                 return student_output['hidden_states'][-1]
#             elif 'last_hidden_state' in student_output:
#                 return student_output['last_hidden_state']''''

        if len(student_output.shape) == 3:
            return student_output
        elif len(student_output.shape) == 2:
            return student_output.unsqueeze(1)
        else:
            return student_output.flatten(start_dim=1).unsqueeze(1)

    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.student_model.train()
        if self.feature_aligner is not None:
            self.feature_aligner.train()

        epoch_losses = {'total_loss': 0.0, 'hard_loss': 0.0, 'soft_loss': 0.0, 'feature_loss': 0.0}
        num_batches = 0

        pbar = tqdm(train_loader, desc=f"Epoch {self.current_epoch + 1}/{self.config.epochs}")

        for batch_idx, batch in enumerate(pbar):
            images = batch['pixel_values'].to(self.device)
            labels = batch['labels'].to(self.device)

            # æå–æ•™å¸ˆç‰¹å¾
            teacher_features = self.extract_teacher_features(images)

            # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
            student_output = self.student_model(images)
            losses = self.compute_distillation_loss(student_output, teacher_features, labels)
            loss = losses['total_loss'] / self.config.grad_accum_steps

            # åå‘ä¼ æ’­
            loss.backward()

            # æ¢¯åº¦ç´¯ç§¯
            if (batch_idx + 1) % self.config.grad_accum_steps == 0:
                torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), self.config.max_grad_norm)
                self.optimizer.step()
                if self.scheduler is not None:
                    self.scheduler.step()
                self.optimizer.zero_grad()
                self.global_step += 1

            # è®°å½•æŸå¤±
            for key in epoch_losses:
                if key in losses:
                    epoch_losses[key] += losses[key].item()
            num_batches += 1

            pbar.set_postfix({'loss': f"{losses['total_loss'].item():.4f}",
                            'lr': f"{self.optimizer.param_groups[0]['lr']:.6f}"})

        for key in epoch_losses:
            epoch_losses[key] /= num_batches

        return epoch_losses

    @torch.no_grad()
    def evaluate(self, val_loader: DataLoader) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.student_model.eval()

        total_loss = 0.0
        correct = 0
        total = 0

        for batch in tqdm(val_loader, desc="Evaluating"):
            images = batch['pixel_values'].to(self.device)
            labels = batch['labels'].to(self.device)

            outputs = self.student_model(images)

            if isinstance(outputs, dict):
                logits = outputs['logits']
            else:
                logits = outputs

            loss = self.ce_loss(logits, labels)
            total_loss += loss.item()

            _, predicted = torch.max(logits, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        avg_loss = total_loss / len(val_loader)
        accuracy = 100.0 * correct / total

        return {'val_loss': avg_loss, 'val_accuracy': accuracy}

    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\n{'='*60}")
        print("ğŸš€ å¼€å§‹è®­ç»ƒ - Qwen2.5-VLå¤šæ¨¡å‹ååŒè®­ç»ƒ")
        print(f"{'='*60}")
        print(f"æ•™å¸ˆæ¨¡å‹: Qwen2.5-VL 3B")
        print(f"å­¦ç”Ÿæ¨¡å‹: {self.config.student_model_type}-{self.config.student_model_size}")
        print(f"ä»»åŠ¡ç±»å‹: {self.config.task_type}")
        print(f"è’¸é¦ç­–ç•¥: {self.config.distillation_type}")
        print(f"è®­ç»ƒè½®æ•°: {self.config.epochs}")
        print(f"æ‰¹å¤§å°: {self.config.batch_size}")
        print(f"å­¦ä¹ ç‡: {self.config.learning_rate}")
        print(f"{'='*60}\n")

        for epoch in range(self.config.epochs):
            self.current_epoch = epoch

            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader)

            # è¯„ä¼°
            val_metrics = self.evaluate(val_loader)

            # æ‰“å°ç»“æœ
            print(f"\nEpoch {epoch + 1}/{self.config.epochs} å®Œæˆ:")
            print(f"  è®­ç»ƒæŸå¤±: {train_metrics['total_loss']:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_metrics['val_loss']:.4f}")
            print(f"  éªŒè¯å‡†ç¡®ç‡: {val_metrics['val_accuracy']:.2f}%")

            # å›è°ƒåç«¯API
            self._update_training_progress(epoch + 1, train_metrics, val_metrics)

            # ä¿å­˜checkpoint
            if self.config.auto_save_checkpoint and (epoch + 1) % self.config.checkpoint_interval == 0:
                self.save_checkpoint(epoch + 1, val_metrics['val_accuracy'])

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['val_accuracy'] > self.best_acc:
                self.best_acc = val_metrics['val_accuracy']
                self.save_checkpoint(epoch + 1, val_metrics['val_accuracy'], is_best=True)

        print(f"\n{'='*60}")
        print("âœ“ è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_acc:.2f}%")
        print(f"{'='*60}\n")

    def save_checkpoint(self, epoch: int, accuracy: float, is_best: bool = False):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.student_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'accuracy': accuracy,
            'config': vars(self.config)
        }

        if self.feature_aligner is not None:
            checkpoint['aligner_state_dict'] = self.feature_aligner.state_dict()

        os.makedirs(self.config.output_dir, exist_ok=True)

        if is_best:
            path = os.path.join(self.config.output_dir, 'best_model.pt')
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {path}")
        else:
            path = os.path.join(self.config.output_dir, f'checkpoint_epoch_{epoch}.pt')
            print(f"ğŸ’¾ ä¿å­˜checkpoint: {path}")

        torch.save(checkpoint, path)

    def _update_training_progress(self, epoch: int, train_metrics: Dict, val_metrics: Dict):
        """å›è°ƒåç«¯APIæ›´æ–°è®­ç»ƒè¿›åº¦"""
        try:
            url = f"{self.config.api_base_url}/model-distillation/tasks/{self.config.task_id}/progress"
            data = {
                'currentEpoch': epoch,
                'totalEpochs': self.config.epochs,
                'trainLoss': train_metrics['total_loss'],
                'valLoss': val_metrics['val_loss'],
                'valAccuracy': val_metrics['val_accuracy'],
                'status': 'RUNNING'
            }

            response = requests.put(url, json=data, timeout=5)
            if response.status_code != 200:
                print(f"âš ï¸  è¿›åº¦æ›´æ–°å¤±è´¥: {response.text}")
        except Exception as e:
            print(f"âš ï¸  è¿›åº¦æ›´æ–°å¼‚å¸¸: {e}")


# ==================== å‘½ä»¤è¡Œå‚æ•°è§£æ ====================

def parse_args():
    parser = argparse.ArgumentParser(description='Qwen2.5-VLå¤šæ¨¡å‹ååŒè®­ç»ƒ')

    # åŸºç¡€é…ç½®
    parser.add_argument('--task_id', type=str, required=True)
    parser.add_argument('--api_base_url', type=str, required=True)

    # æ¨¡å‹é…ç½®
    parser.add_argument('--teacher_model', type=str, required=True)
    parser.add_argument('--student_model', type=str, required=True)
    parser.add_argument('--teacher_path', type=str, required=True)
    parser.add_argument('--student_path', type=str, default=None)

    # å­¦ç”Ÿæ¨¡å‹ç±»å‹å’Œå¤§å°
    parser.add_argument('--student_model_type', type=str, required=True,
                       choices=['resnet', 'vit', 'yolov8', 'unet', 'lstm'])
    parser.add_argument('--student_model_size', type=str, required=True)

    # ä»»åŠ¡é…ç½®
    parser.add_argument('--task_type', type=str, default='classification',
                       choices=['classification', 'detection', 'segmentation'])
    parser.add_argument('--num_classes', type=int, default=10)

    # æ•°æ®é…ç½®
    parser.add_argument('--dataset_id', type=str, required=True)
    parser.add_argument('--val_dataset_id', type=str, default=None)
    parser.add_argument('--image_size', type=int, default=224)

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch_size', type=int, default=1)
    parser.add_argument('--learning_rate', type=float, default=1e-4)

    # ä¼˜åŒ–å™¨é…ç½®
    parser.add_argument('--optimizer', type=str, default='adamw',
                       choices=['adamw', 'adam', 'sgd'])
    parser.add_argument('--lr_scheduler', type=str, default='cosine',
                       choices=['cosine', 'linear', 'constant'])
    parser.add_argument('--weight_decay', type=float, default=0.01)
    parser.add_argument('--grad_accum_steps', type=int, default=1)
    parser.add_argument('--max_grad_norm', type=float, default=1.0)

    # GPUé…ç½®
    parser.add_argument('--gpu_devices', type=str, default='0')
    parser.add_argument('--auto_save_checkpoint', type=bool, default=True)
    #parser.add_argument('--auto_save_checkpoint', action='store_true')
    #parser.add_argument('--no_auto_save_checkpoint', action='store_false',
                        #dest='auto_save_checkpoint')
    parser.add_argument('--checkpoint_interval', type=int, default=10)

    # LoRAé…ç½®
    parser.add_argument('--lora_rank', type=int, default=0)
    parser.add_argument('--lora_alpha', type=int, default=16)
    parser.add_argument('--lora_dropout', type=float, default=0.1)
    parser.add_argument('--lora_target_modules', type=str, default='')
    parser.add_argument('--lora_bias', type=str, default='none')

    # çŸ¥è¯†è’¸é¦é…ç½®
    parser.add_argument('--temperature', type=float, default=4.0)
    parser.add_argument('--hard_label_weight', type=float, default=0.5)
    parser.add_argument('--soft_label_weight', type=float, default=0.5)
    parser.add_argument('--distill_loss_type', type=str, default='kl_div')

    # è’¸é¦ç­–ç•¥
    parser.add_argument('--distillation_type', type=str, default='hybrid',
                       choices=['feature', 'logit', 'hybrid'])
    parser.add_argument('--feature_loss_type', type=str, default='mse',
                       choices=['mse', 'cosine'])
    parser.add_argument('--align_feature', type=bool, default=True)
#     parser.add_argument('--align_feature', action='store_true')
#     parser.add_argument('--no_align_feature', action='store_false', dest='align_feature')
    parser.set_defaults(align_feature=True)

    # è¾“å‡ºé…ç½®
    parser.add_argument('--output_dir', type=str, required=True)

    # æ•°æ®é›†æ ¹ç›®å½•ï¼ˆæ–°å¢ï¼Œç”±åç«¯é…ç½®æ–‡ä»¶ä¼ é€’ï¼‰
    parser.add_argument('--datasets_root', type=str, required=True, help='æ•°æ®é›†æ ¹ç›®å½•')

    return parser.parse_args()


# ==================== ä¸»å‡½æ•° ====================

def main():
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     Qwen2.5-VL å¤šæ¨¡å‹ååŒè®­ç»ƒç³»ç»Ÿ                            â•‘
    â•‘     Multi-Model Collaborative Training with Qwen2.5-VL       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    args = parse_args()
    config = TrainingConfig(args)

    print("\næ­£åœ¨åŠ è½½æ•°æ®é›†...")

    # æ„å»ºè®­ç»ƒé›†è·¯å¾„
    train_dataset_path = os.path.join(config.datasets_root, config.dataset_id, "train")
    print(f"è®­ç»ƒé›†è·¯å¾„: {train_dataset_path}")

    # æ„å»ºéªŒè¯é›†è·¯å¾„
    val_dataset_id = config.val_dataset_id or config.dataset_id
    val_dataset_path = os.path.join(config.datasets_root, val_dataset_id, "val")
    print(f"éªŒè¯é›†è·¯å¾„: {val_dataset_path}")

    train_dataset = MultiTaskDataset(
        train_dataset_path,
        config.task_type,
        config.image_size,
        config.num_classes,
        mode='train'
    )
    val_dataset = MultiTaskDataset(
        val_dataset_path,
        config.task_type,
        config.image_size,
        config.num_classes,
        mode='val'
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )

    print(f"âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"âœ“ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")

    trainer = QwenMultiModelDistillationTrainer(config)
    trainer.train(train_loader, val_loader)

    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")


if __name__ == '__main__':
    main()