#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤§å°æ¨¡å‹ååŒè®­ç»ƒï¼ˆçŸ¥è¯†è’¸é¦ï¼‰è®­ç»ƒè„šæœ¬

åŠŸèƒ½ï¼š
1. ä»å‘½ä»¤è¡Œå‚æ•°è¯»å–æ‰€æœ‰è®­ç»ƒé…ç½®
2. åŠ è½½æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹
3. åº”ç”¨LoRAå¾®è°ƒé…ç½®
4. å®ç°çŸ¥è¯†è’¸é¦è®­ç»ƒå¾ªç¯
5. å®šæœŸå›è°ƒåç«¯APIæ›´æ–°è®­ç»ƒè¿›åº¦
6. ä¿å­˜è®­ç»ƒcheckpointså’Œæœ€ç»ˆæ¨¡å‹

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-01-25
"""

import argparse
import json
import os
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import requests
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import (
    AutoConfig,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    get_cosine_schedule_with_warmup,
    get_linear_schedule_with_warmup,
)
from peft import LoraConfig, get_peft_model, TaskType

# ==================== é…ç½®ç±» ====================

class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨æ‰€æœ‰è®­ç»ƒå‚æ•°"""

    def __init__(self, args):
        # åŸºç¡€é…ç½®
        self.task_id = args.task_id
        self.api_base_url = args.api_base_url

        # æ¨¡å‹é…ç½®
        self.teacher_model = args.teacher_model
        self.student_model = args.student_model
        self.teacher_path = args.teacher_path
        self.student_path = args.student_path

        # æ•°æ®é…ç½®
        self.dataset_id = args.dataset_id
        self.val_dataset_id = args.val_dataset_id

        # è®­ç»ƒå‚æ•°
        self.epochs = args.epochs
        self.batch_size = args.batch_size
        self.learning_rate = args.learning_rate

        # ä¼˜åŒ–å™¨é…ç½®
        self.optimizer = args.optimizer  # adamw, adam, sgd
        self.lr_scheduler = args.lr_scheduler  # cosine, linear, constant
        self.weight_decay = args.weight_decay
        self.grad_accum_steps = args.grad_accum_steps
        self.max_grad_norm = args.max_grad_norm

        # GPUé…ç½®
        self.gpu_devices = self._parse_gpu_devices(args.gpu_devices)
        self.auto_save_checkpoint = args.auto_save_checkpoint
        self.checkpoint_interval = args.checkpoint_interval

        # LoRAé…ç½®
        self.lora_rank = args.lora_rank
        self.lora_alpha = args.lora_alpha
        self.lora_dropout = args.lora_dropout
        self.lora_target_modules = self._parse_list(args.lora_target_modules)
        self.lora_bias = args.lora_bias

        # çŸ¥è¯†è’¸é¦é…ç½®
        self.temperature = args.temperature
        self.hard_label_weight = args.hard_label_weight
        self.soft_label_weight = args.soft_label_weight
        self.distill_loss_type = args.distill_loss_type  # kl_div, mse, cosine

        # è¾“å‡ºé…ç½®
        self.output_dir = args.output_dir

    def _parse_gpu_devices(self, gpu_str: str) -> List[int]:
        """è§£æGPUè®¾å¤‡åˆ—è¡¨"""
        if not gpu_str or gpu_str == "":
            return [0]
        return [int(x.strip()) for x in gpu_str.split(",")]

    def _parse_list(self, list_str: str) -> List[str]:
        """è§£æé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²åˆ—è¡¨"""
        if not list_str or list_str == "":
            return []
        return [x.strip() for x in list_str.split(",")]


# ==================== æ•°æ®é›†ç±» ====================

class DummyDataset(Dataset):
    """
    ç¤ºä¾‹æ•°æ®é›†ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦æ›¿æ¢ä¸ºçœŸå®æ•°æ®é›†ï¼‰
    è¿™é‡Œä½¿ç”¨éšæœºæ•°æ®è¿›è¡Œæ¼”ç¤º
    """

    def __init__(self, num_samples: int = 1000, max_length: int = 128):
        self.num_samples = num_samples
        self.max_length = max_length

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # ç”Ÿæˆéšæœºè¾“å…¥IDå’Œæ ‡ç­¾
        input_ids = torch.randint(0, 1000, (self.max_length,))
        attention_mask = torch.ones(self.max_length)
        label = torch.randint(0, 2, (1,)).item()

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': label
        }


# ==================== çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨ ====================

class DistillationTrainer:
    """çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨"""

    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = self._setup_device()
        self.tokenizer = None
        self.teacher_model = None
        self.student_model = None
        self.optimizer = None
        self.scheduler = None

    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
        if torch.cuda.is_available():
            # ä½¿ç”¨é…ç½®çš„ç¬¬ä¸€ä¸ªGPU
            device_id = self.config.gpu_devices[0]
            device = torch.device(f"cuda:{device_id}")
            print(f"âœ“ ä½¿ç”¨GPUè®¾å¤‡: cuda:{device_id}")
        else:
            device = torch.device("cpu")
            print("âš  ä½¿ç”¨CPUè®­ç»ƒï¼ˆå»ºè®®ä½¿ç”¨GPUï¼‰")
        return device

    def load_models(self):
        """åŠ è½½æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹"""
        print(f"\n{'='*60}")
        print("ğŸ“š åŠ è½½æ¨¡å‹...")
        print(f"{'='*60}")

        # åŠ è½½åˆ†è¯å™¨
        print(f"æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {self.config.teacher_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.teacher_path,
            trust_remote_code=True
        )

        # åŠ è½½æ•™å¸ˆæ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½æ•™å¸ˆæ¨¡å‹: {self.config.teacher_path}")
        teacher_config = AutoConfig.from_pretrained(self.config.teacher_path)
        teacher_config.num_labels = 2  # äºŒåˆ†ç±»ä»»åŠ¡

        self.teacher_model = AutoModelForSequenceClassification.from_pretrained(
            self.config.teacher_path,
            config=teacher_config,
            trust_remote_code=True
        )
        self.teacher_model.to(self.device)
        self.teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹è®¾ä¸ºè¯„ä¼°æ¨¡å¼

        # å†»ç»“æ•™å¸ˆæ¨¡å‹å‚æ•°
        for param in self.teacher_model.parameters():
            param.requires_grad = False

        print(f"âœ“ æ•™å¸ˆæ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in self.teacher_model.parameters()):,}")

        # åŠ è½½å­¦ç”Ÿæ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½å­¦ç”Ÿæ¨¡å‹: {self.config.student_path or 'éšæœºåˆå§‹åŒ–'}")
        student_config = AutoConfig.from_pretrained(
            self.config.student_path or self.config.teacher_path
        )
        student_config.num_labels = 2

        # æ ¹æ®é…ç½®åˆ›å»ºæ›´å°çš„å­¦ç”Ÿæ¨¡å‹
        if not self.config.student_path:
            student_config.num_hidden_layers = 6  # å‡å°‘å±‚æ•°
            student_config.hidden_size = 384  # å‡å°‘éšè—å±‚å¤§å°
            student_config.num_attention_heads = 6

        self.student_model = AutoModelForSequenceClassification.from_config(
            student_config
        )

        # åº”ç”¨LoRAé…ç½®
        self._apply_lora()

        self.student_model.to(self.device)

        print(f"âœ“ å­¦ç”Ÿæ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in self.student_model.parameters()):,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in self.student_model.parameters() if p.requires_grad):,}")

    def _apply_lora(self):
        """åº”ç”¨LoRAé…ç½®åˆ°å­¦ç”Ÿæ¨¡å‹"""
        print(f"\nğŸ”§ åº”ç”¨LoRAé…ç½®...")
        print(f"  Rank: {self.config.lora_rank}")
        print(f"  Alpha: {self.config.lora_alpha}")
        print(f"  Dropout: {self.config.lora_dropout}")
        print(f"  Target Modules: {self.config.lora_target_modules or 'default'}")

        lora_config = LoraConfig(
            task_type=TaskType.SEQ_CLS,
            inference_mode=False,
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=self.config.lora_target_modules if self.config.lora_target_modules else None,
            bias=self.config.lora_bias,
        )

        self.student_model = get_peft_model(self.student_model, lora_config)
        self.student_model.print_trainable_parameters()

    def setup_optimizer(self, num_training_steps: int):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        print(f"\nâš™ï¸ è®¾ç½®ä¼˜åŒ–å™¨...")

        # é€‰æ‹©ä¼˜åŒ–å™¨
        if self.config.optimizer == "adamw":
            self.optimizer = torch.optim.AdamW(
                self.student_model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer == "adam":
            self.optimizer = torch.optim.Adam(
                self.student_model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer == "sgd":
            self.optimizer = torch.optim.SGD(
                self.student_model.parameters(),
                lr=self.config.learning_rate,
                momentum=0.9,
                weight_decay=self.config.weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.config.optimizer}")

        print(f"âœ“ ä¼˜åŒ–å™¨: {self.config.optimizer.upper()}")
        print(f"  å­¦ä¹ ç‡: {self.config.learning_rate}")
        print(f"  æƒé‡è¡°å‡: {self.config.weight_decay}")

        # é€‰æ‹©å­¦ä¹ ç‡è°ƒåº¦å™¨
        num_warmup_steps = int(0.1 * num_training_steps)  # 10% warmup

        if self.config.lr_scheduler == "cosine":
            self.scheduler = get_cosine_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=num_warmup_steps,
                num_training_steps=num_training_steps
            )
        elif self.config.lr_scheduler == "linear":
            self.scheduler = get_linear_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=num_warmup_steps,
                num_training_steps=num_training_steps
            )
        else:  # constant
            self.scheduler = None

        print(f"âœ“ å­¦ä¹ ç‡è°ƒåº¦å™¨: {self.config.lr_scheduler or 'constant'}")

    def compute_distillation_loss(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        labels: torch.Tensor
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        è®¡ç®—çŸ¥è¯†è’¸é¦æŸå¤±

        Returns:
            total_loss: æ€»æŸå¤±
            loss_dict: æŸå¤±è¯¦æƒ…å­—å…¸
        """
        # ç¡¬æ ‡ç­¾æŸå¤±ï¼ˆçœŸå®æ ‡ç­¾ï¼‰
        hard_loss = F.cross_entropy(student_logits, labels)

        # è½¯æ ‡ç­¾æŸå¤±ï¼ˆæ•™å¸ˆè¾“å‡ºï¼‰
        if self.config.distill_loss_type == "kl_div":
            # KLæ•£åº¦æŸå¤±
            soft_loss = F.kl_div(
                F.log_softmax(student_logits / self.config.temperature, dim=-1),
                F.softmax(teacher_logits / self.config.temperature, dim=-1),
                reduction='batchmean'
            ) * (self.config.temperature ** 2)
        elif self.config.distill_loss_type == "mse":
            # MSEæŸå¤±
            soft_loss = F.mse_loss(
                F.softmax(student_logits / self.config.temperature, dim=-1),
                F.softmax(teacher_logits / self.config.temperature, dim=-1)
            )
        else:
            # é»˜è®¤ä½¿ç”¨KLæ•£åº¦
            soft_loss = F.kl_div(
                F.log_softmax(student_logits / self.config.temperature, dim=-1),
                F.softmax(teacher_logits / self.config.temperature, dim=-1),
                reduction='batchmean'
            ) * (self.config.temperature ** 2)

        # ç»„åˆæŸå¤±
        total_loss = (
            self.config.hard_label_weight * hard_loss +
            self.config.soft_label_weight * soft_loss
        )

        loss_dict = {
            'total_loss': total_loss.item(),
            'hard_loss': hard_loss.item(),
            'soft_loss': soft_loss.item()
        }

        return total_loss, loss_dict

    def train_epoch(
        self,
        train_loader: DataLoader,
        epoch: int
    ) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.student_model.train()

        total_loss = 0
        total_hard_loss = 0
        total_soft_loss = 0
        correct = 0
        total = 0

        num_batches = len(train_loader)

        for batch_idx, batch in enumerate(train_loader):
            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
            with torch.no_grad():
                teacher_outputs = self.teacher_model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                teacher_logits = teacher_outputs.logits

            # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
            student_outputs = self.student_model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            student_logits = student_outputs.logits

            # è®¡ç®—æŸå¤±
            loss, loss_dict = self.compute_distillation_loss(
                student_logits, teacher_logits, labels
            )

            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / self.config.grad_accum_steps
            loss.backward()

            # æ¢¯åº¦ç´¯ç§¯åæ›´æ–°
            if (batch_idx + 1) % self.config.grad_accum_steps == 0:
                # æ¢¯åº¦è£å‰ª
                if self.config.max_grad_norm > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.student_model.parameters(),
                        self.config.max_grad_norm
                    )

                self.optimizer.step()
                if self.scheduler:
                    self.scheduler.step()
                self.optimizer.zero_grad()

            # ç»Ÿè®¡
            total_loss += loss_dict['total_loss']
            total_hard_loss += loss_dict['hard_loss']
            total_soft_loss += loss_dict['soft_loss']

            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(student_logits, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % 10 == 0:
                avg_loss = total_loss / (batch_idx + 1)
                accuracy = 100.0 * correct / total
                print(f"  Batch [{batch_idx+1}/{num_batches}] "
                      f"Loss: {avg_loss:.4f} | Acc: {accuracy:.2f}%")

        # Epochç»Ÿè®¡
        epoch_metrics = {
            'loss': total_loss / num_batches,
            'hard_loss': total_hard_loss / num_batches,
            'soft_loss': total_soft_loss / num_batches,
            'accuracy': 100.0 * correct / total
        }

        return epoch_metrics

    @torch.no_grad()
    def evaluate(self, val_loader: DataLoader) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹"""
        self.student_model.eval()

        total_loss = 0
        correct = 0
        total = 0

        for batch in val_loader:
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
            outputs = self.student_model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            logits = outputs.logits

            # è®¡ç®—æŸå¤±
            loss = F.cross_entropy(logits, labels)
            total_loss += loss.item()

            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(logits, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        metrics = {
            'val_loss': total_loss / len(val_loader),
            'val_accuracy': 100.0 * correct / total
        }

        return metrics

    def save_checkpoint(self, epoch: int, metrics: Dict[str, float]):
        """ä¿å­˜checkpoint"""
        checkpoint_dir = os.path.join(self.config.output_dir, f"checkpoint-epoch-{epoch}")
        os.makedirs(checkpoint_dir, exist_ok=True)

        # ä¿å­˜æ¨¡å‹
        self.student_model.save_pretrained(checkpoint_dir)
        self.tokenizer.save_pretrained(checkpoint_dir)

        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        state = {
            'epoch': epoch,
            'metrics': metrics,
            'optimizer_state': self.optimizer.state_dict(),
        }

        if self.scheduler:
            state['scheduler_state'] = self.scheduler.state_dict()

        torch.save(state, os.path.join(checkpoint_dir, 'training_state.pt'))

        print(f"âœ“ Checkpointå·²ä¿å­˜: {checkpoint_dir}")

    def update_progress_to_backend(
        self,
        epoch: int,
        metrics: Dict[str, float]
    ):
        """å›è°ƒåç«¯APIæ›´æ–°è®­ç»ƒè¿›åº¦"""
        try:
            url = f"{self.config.api_base_url}/model-distillation/tasks/{self.config.task_id}/progress"

            params = {
                'currentEpoch': epoch,
                'accuracy': metrics.get('accuracy', 0),
                'loss': metrics.get('loss', 0)
            }

            response = requests.put(url, params=params, timeout=10)

            if response.status_code == 200:
                print(f"âœ“ è¿›åº¦å·²æ›´æ–°åˆ°åç«¯ (Epoch {epoch})")
            else:
                print(f"âš  æ›´æ–°è¿›åº¦å¤±è´¥: {response.status_code}")

        except Exception as e:
            print(f"âš  æ›´æ–°è¿›åº¦å¼‚å¸¸: {str(e)}")

    def complete_task(self, final_metrics: Dict[str, float]):
        """æ ‡è®°ä»»åŠ¡å®Œæˆ"""
        try:
            url = f"{self.config.api_base_url}/model-distillation/tasks/{self.config.task_id}/complete"
            response = requests.post(url, timeout=10)

            if response.status_code == 200:
                print(f"\nâœ“ è®­ç»ƒä»»åŠ¡å·²å®Œæˆï¼")
                print(f"  æœ€ç»ˆå‡†ç¡®ç‡: {final_metrics.get('accuracy', 0):.2f}%")
            else:
                print(f"âš  æ ‡è®°å®Œæˆå¤±è´¥: {response.status_code}")

        except Exception as e:
            print(f"âš  æ ‡è®°å®Œæˆå¼‚å¸¸: {str(e)}")

    def report_error(self, error_message: str):
        """æŠ¥å‘Šè®­ç»ƒé”™è¯¯"""
        try:
            url = f"{self.config.api_base_url}/model-distillation/tasks/{self.config.task_id}/error"
            params = {'errorMessage': error_message}
            requests.put(url, params=params, timeout=10)
        except Exception as e:
            print(f"âš  æŠ¥å‘Šé”™è¯¯å¼‚å¸¸: {str(e)}")

    def train(self):
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒä»»åŠ¡: {self.config.task_id}")
        print(f"{'='*60}")

        try:
            # åŠ è½½æ¨¡å‹
            self.load_models()

            # å‡†å¤‡æ•°æ®é›†ï¼ˆè¿™é‡Œä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼Œå®é™…éœ€è¦æ›¿æ¢ï¼‰
            print(f"\nğŸ“Š å‡†å¤‡æ•°æ®é›†...")
            train_dataset = DummyDataset(num_samples=1000)
            val_dataset = DummyDataset(num_samples=200)

            train_loader = DataLoader(
                train_dataset,
                batch_size=self.config.batch_size,
                shuffle=True,
                num_workers=2
            )

            val_loader = DataLoader(
                val_dataset,
                batch_size=self.config.batch_size,
                shuffle=False,
                num_workers=2
            )

            print(f"âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
            print(f"âœ“ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")

            # è®¾ç½®ä¼˜åŒ–å™¨
            num_training_steps = len(train_loader) * self.config.epochs
            self.setup_optimizer(num_training_steps)

            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(self.config.output_dir, exist_ok=True)

            # è®­ç»ƒå¾ªç¯
            print(f"\n{'='*60}")
            print(f"ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ ({self.config.epochs} epochs)")
            print(f"{'='*60}\n")

            best_accuracy = 0

            for epoch in range(1, self.config.epochs + 1):
                print(f"\nğŸ“ Epoch {epoch}/{self.config.epochs}")
                print("-" * 60)

                # è®­ç»ƒ
                train_metrics = self.train_epoch(train_loader, epoch)

                # éªŒè¯
                val_metrics = self.evaluate(val_loader)

                # åˆå¹¶æŒ‡æ ‡
                all_metrics = {**train_metrics, **val_metrics}

                # æ‰“å°epochç»“æœ
                print(f"\nğŸ“ˆ Epoch {epoch} ç»“æœ:")
                print(f"  è®­ç»ƒæŸå¤±: {train_metrics['loss']:.4f}")
                print(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_metrics['accuracy']:.2f}%")
                print(f"  éªŒè¯æŸå¤±: {val_metrics['val_loss']:.4f}")
                print(f"  éªŒè¯å‡†ç¡®ç‡: {val_metrics['val_accuracy']:.2f}%")

                # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
                if val_metrics['val_accuracy'] > best_accuracy:
                    best_accuracy = val_metrics['val_accuracy']
                    print(f"  ğŸ¯ æ–°çš„æœ€ä½³å‡†ç¡®ç‡!")

                # ä¿å­˜checkpoint
                if self.config.auto_save_checkpoint and epoch % self.config.checkpoint_interval == 0:
                    self.save_checkpoint(epoch, all_metrics)

                # å›è°ƒåç«¯æ›´æ–°è¿›åº¦
                self.update_progress_to_backend(epoch, all_metrics)

            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
            final_model_dir = os.path.join(self.config.output_dir, "final_model")
            self.student_model.save_pretrained(final_model_dir)
            self.tokenizer.save_pretrained(final_model_dir)
            print(f"âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_dir}")

            # æ ‡è®°ä»»åŠ¡å®Œæˆ
            final_metrics = {'accuracy': best_accuracy}
            self.complete_task(final_metrics)

            print(f"\n{'='*60}")
            print(f"ğŸ‰ è®­ç»ƒå®Œæˆ!")
            print(f"  æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")
            print(f"  æ¨¡å‹ä¿å­˜è·¯å¾„: {final_model_dir}")
            print(f"{'='*60}\n")

        except Exception as e:
            error_msg = f"è®­ç»ƒå¤±è´¥: {str(e)}"
            print(f"\nâŒ {error_msg}")
            import traceback
            traceback.print_exc()
            self.report_error(error_msg)
            sys.exit(1)


# ==================== ä¸»å‡½æ•° ====================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="å¤§å°æ¨¡å‹ååŒè®­ç»ƒè„šæœ¬")

    # åŸºç¡€é…ç½®
    parser.add_argument("--task_id", type=str, required=True, help="ä»»åŠ¡ID")
    parser.add_argument("--api_base_url", type=str, required=True, help="åç«¯APIåœ°å€")

    # æ¨¡å‹é…ç½®
    parser.add_argument("--teacher_model", type=str, required=True, help="æ•™å¸ˆæ¨¡å‹åç§°")
    parser.add_argument("--student_model", type=str, required=True, help="å­¦ç”Ÿæ¨¡å‹åç§°")
    parser.add_argument("--teacher_path", type=str, required=True, help="æ•™å¸ˆæ¨¡å‹è·¯å¾„")
    parser.add_argument("--student_path", type=str, default="", help="å­¦ç”Ÿæ¨¡å‹è·¯å¾„")

    # æ•°æ®é…ç½®
    parser.add_argument("--dataset_id", type=str, required=True, help="æ•°æ®é›†ID")
    parser.add_argument("--val_dataset_id", type=str, default="", help="éªŒè¯æ•°æ®é›†ID")

    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=0.0001, help="å­¦ä¹ ç‡")

    # ä¼˜åŒ–å™¨é…ç½®
    parser.add_argument("--optimizer", type=str, default="adamw", help="ä¼˜åŒ–å™¨")
    parser.add_argument("--lr_scheduler", type=str, default="cosine", help="å­¦ä¹ ç‡è°ƒåº¦å™¨")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    parser.add_argument("--grad_accum_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="æœ€å¤§æ¢¯åº¦èŒƒæ•°")

    # GPUé…ç½®
    parser.add_argument("--gpu_devices", type=str, default="0", help="GPUè®¾å¤‡")
    parser.add_argument("--auto_save_checkpoint", type=bool, default=True, help="è‡ªåŠ¨ä¿å­˜checkpoint")
    parser.add_argument("--checkpoint_interval", type=int, default=5, help="checkpointä¿å­˜é—´éš”")

    # LoRAé…ç½®
    parser.add_argument("--lora_rank", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.05, help="LoRA dropout")
    parser.add_argument("--lora_target_modules", type=str, default="", help="LoRAç›®æ ‡æ¨¡å—")
    parser.add_argument("--lora_bias", type=str, default="none", help="LoRA bias")

    # çŸ¥è¯†è’¸é¦é…ç½®
    parser.add_argument("--temperature", type=float, default=3.0, help="è’¸é¦æ¸©åº¦")
    parser.add_argument("--hard_label_weight", type=float, default=0.3, help="ç¡¬æ ‡ç­¾æƒé‡")
    parser.add_argument("--soft_label_weight", type=float, default=0.7, help="è½¯æ ‡ç­¾æƒé‡")
    parser.add_argument("--distill_loss_type", type=str, default="kl_div", help="è’¸é¦æŸå¤±ç±»å‹")

    # è¾“å‡ºé…ç½®
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºç›®å½•")

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    args = parse_args()

    # åˆ›å»ºé…ç½®
    config = TrainingConfig(args)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DistillationTrainer(config)

    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == "__main__":
    main()
