#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤§å°æ¨¡å‹ååŒè®­ç»ƒï¼ˆçŸ¥è¯†è’¸é¦ï¼‰è®­ç»ƒè„šæœ¬

åŠŸèƒ½ï¼š
1. ä»å‘½ä»¤è¡Œå‚æ•°è¯»å–æ‰€æœ‰è®­ç»ƒé…ç½®
2. åŠ è½½æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹
3. åº”ç”¨LoRAå¾®è°ƒé…ç½®
4. å®ç°çŸ¥è¯†è’¸é¦è®­ç»ƒå¾ªç¯
5. å®šæœŸå›è°ƒåç«¯APIæ›´æ–°è®­ç»ƒè¿›åº¦
6. ä¿å­˜è®­ç»ƒcheckpointså’Œæœ€ç»ˆæ¨¡å‹

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-01-25
"""

import argparse
import json
import os
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import requests
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
from transformers import (
    AutoConfig,
    AutoModelForImageClassification,
    AutoImageProcessor,
    get_cosine_schedule_with_warmup,
    get_linear_schedule_with_warmup,
)
from peft import LoraConfig, get_peft_model, TaskType

# ==================== é…ç½®ç±» ====================

class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨æ‰€æœ‰è®­ç»ƒå‚æ•°"""

    def __init__(self, args):
        # åŸºç¡€é…ç½®
        self.task_id = args.task_id
        self.api_base_url = args.api_base_url

        # æ¨¡å‹é…ç½®
        self.teacher_model = args.teacher_model
        self.student_model = args.student_model
        self.teacher_path = args.teacher_path
        self.student_path = args.student_path

        # æ•°æ®é…ç½®
        self.dataset_id = args.dataset_id
        self.val_dataset_id = args.val_dataset_id
        self.datasets_root = args.datasets_root

        # è®­ç»ƒå‚æ•°
        self.epochs = args.epochs
        self.batch_size = args.batch_size
        self.learning_rate = args.learning_rate

        # ä¼˜åŒ–å™¨é…ç½®
        self.optimizer = args.optimizer  # adamw, adam, sgd
        self.lr_scheduler = args.lr_scheduler  # cosine, linear, constant
        self.weight_decay = args.weight_decay
        self.grad_accum_steps = args.grad_accum_steps
        self.max_grad_norm = args.max_grad_norm

        # GPUé…ç½®
        self.gpu_devices = self._parse_gpu_devices(args.gpu_devices)
        self.auto_save_checkpoint = args.auto_save_checkpoint
        self.checkpoint_interval = args.checkpoint_interval

        # LoRAé…ç½®
        self.lora_rank = args.lora_rank
        self.lora_alpha = args.lora_alpha
        self.lora_dropout = args.lora_dropout
        self.lora_target_modules = self._parse_list(args.lora_target_modules)
        self.lora_bias = args.lora_bias

        # çŸ¥è¯†è’¸é¦é…ç½®
        self.temperature = args.temperature
        self.hard_label_weight = args.hard_label_weight
        self.soft_label_weight = args.soft_label_weight
        self.distill_loss_type = args.distill_loss_type  # kl_div, mse, cosine

        # è¾“å‡ºé…ç½®
        self.output_dir = args.output_dir

    def _parse_gpu_devices(self, gpu_str: str) -> List[int]:
        """è§£æGPUè®¾å¤‡åˆ—è¡¨"""
        if not gpu_str or gpu_str == "":
            return [0]
        return [int(x.strip()) for x in gpu_str.split(",")]

    def _parse_list(self, list_str: str) -> List[str]:
        """è§£æé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²åˆ—è¡¨"""
        if not list_str or list_str == "":
            return []
        return [x.strip() for x in list_str.split(",")]


# ==================== æ•°æ®é›†ç±» ====================

class ImageAnnotationDataset(Dataset):
    """
    å›¾åƒæ ‡æ³¨æ•°æ®é›†
    æ”¯æŒï¼šå›¾åƒåˆ†ç±»ï¼ˆå¦‚CIFAR-10ï¼‰

    æœŸæœ›çš„ç›®å½•ç»“æ„ï¼š
    dataset_path/
      â”œâ”€â”€ class1/
      â”‚   â”œâ”€â”€ img1.jpg
      â”‚   â””â”€â”€ img2.jpg
      â”œâ”€â”€ class2/
      â”‚   â””â”€â”€ ...
    """

    def __init__(
        self,
        dataset_path: str,
        image_processor=None,
        num_samples: int = None,
        image_size: int = 224,
        num_classes: int = None,
        is_training: bool = True
    ):
        self.dataset_path = dataset_path
        self.image_processor = image_processor
        self.image_size = image_size
        self.num_classes = num_classes
        self.is_training = is_training

        # å­˜å‚¨æ‰€æœ‰å›¾åƒè·¯å¾„å’Œæ ‡ç­¾
        self.image_paths = []
        self.labels = []
        self.class_names = []

        # æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦å­˜åœ¨
        if os.path.exists(dataset_path):
            self._load_dataset()
        else:
            print(f"âš ï¸ è­¦å‘Š: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
            print(f"ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º")
            self._use_mock_data(num_samples or 1000)

        # æ•°æ®å¢å¼ºï¼ˆè®­ç»ƒé›†ï¼‰
        if self.is_training:
            self.transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.RandomHorizontalFlip(),
                transforms.RandomRotation(15),
                transforms.ColorJitter(brightness=0.2, contrast=0.2),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])
        else:
            # éªŒè¯é›†ä¸ä½¿ç”¨æ•°æ®å¢å¼º
            self.transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                   std=[0.229, 0.224, 0.225])
            ])

    def _load_dataset(self):
        """ä»ç›®å½•ç»“æ„åŠ è½½çœŸå®æ•°æ®é›†"""
        print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {self.dataset_path}")

        # è·å–æ‰€æœ‰ç±»åˆ«æ–‡ä»¶å¤¹
        class_folders = sorted([d for d in os.listdir(self.dataset_path)
                               if os.path.isdir(os.path.join(self.dataset_path, d))])

        if not class_folders:
            print(f"âš ï¸ è­¦å‘Š: åœ¨ {self.dataset_path} ä¸­æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶å¤¹")
            self._use_mock_data(1000)
            return

        self.class_names = class_folders
        print(f"âœ… æ‰¾åˆ° {len(self.class_names)} ä¸ªç±»åˆ«: {self.class_names}")

        # éå†æ¯ä¸ªç±»åˆ«æ–‡ä»¶å¤¹
        for class_idx, class_name in enumerate(self.class_names):
            class_dir = os.path.join(self.dataset_path, class_name)

            # è·å–è¯¥ç±»åˆ«ä¸‹çš„æ‰€æœ‰å›¾åƒæ–‡ä»¶
            image_files = [f for f in os.listdir(class_dir)
                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]

            for img_file in image_files:
                img_path = os.path.join(class_dir, img_file)
                self.image_paths.append(img_path)
                self.labels.append(class_idx)

        print(f"âœ… åŠ è½½å®Œæˆ: å…± {len(self.image_paths)} å¼ å›¾åƒ")

        # æ›´æ–°ç±»åˆ«æ•°
        if self.num_classes is None:
            self.num_classes = len(self.class_names)

    def _use_mock_data(self, num_samples):
        """ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆå½“çœŸå®æ•°æ®ä¸å¯ç”¨æ—¶ï¼‰"""
        print(f"ğŸ­ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {num_samples} ä¸ªæ ·æœ¬")
        self.class_names = [f"class_{i}" for i in range(self.num_classes or 10)]

        # ç”Ÿæˆæ¨¡æ‹Ÿè·¯å¾„å’Œæ ‡ç­¾
        for i in range(num_samples):
            self.image_paths.append(f"mock_image_{i}.jpg")
            self.labels.append(i % len(self.class_names))

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]

        # å°è¯•åŠ è½½çœŸå®å›¾åƒ
        try:
            if os.path.exists(img_path):
                image = Image.open(img_path).convert('RGB')
            else:
                # ç”Ÿæˆéšæœºå›¾åƒï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
                import numpy as np
                image_array = np.random.randint(0, 255, (self.image_size, self.image_size, 3), dtype=np.uint8)
                image = Image.fromarray(image_array)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å›¾åƒå¤±è´¥ {img_path}: {e}")
            # ç”Ÿæˆéšæœºå›¾åƒä½œä¸ºåå¤‡
            import numpy as np
            image_array = np.random.randint(0, 255, (self.image_size, self.image_size, 3), dtype=np.uint8)
            image = Image.fromarray(image_array)

        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)

        return image, label

        # åº”ç”¨å˜æ¢
        pixel_values = self.transform(image)

        # æ ‡ç­¾ï¼ˆè¿™é‡Œç”¨éšæœºæ ‡ç­¾æ¼”ç¤ºï¼‰
        # å®é™…åº”è¯¥ä»æ•°æ®åº“è¯»å–çœŸå®æ ‡æ³¨
        if self.num_classes:
            label = torch.randint(0, self.num_classes, (1,)).item()
        else:
            label = torch.randint(0, 10, (1,)).item()  # é»˜è®¤10ä¸ªç±»åˆ«

        return {
            'pixel_values': pixel_values,
            'labels': label
        }


# ==================== çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨ ====================

class DistillationTrainer:
    """çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨"""

    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = self._setup_device()
        self.tokenizer = None
        self.teacher_model = None
        self.student_model = None
        self.optimizer = None
        self.scheduler = None

    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
        if torch.cuda.is_available():
            # ä½¿ç”¨é…ç½®çš„ç¬¬ä¸€ä¸ªGPU
            device_id = self.config.gpu_devices[0]
            device = torch.device(f"cuda:{device_id}")
            print(f"âœ“ ä½¿ç”¨GPUè®¾å¤‡: cuda:{device_id}")
        else:
            device = torch.device("cpu")
            print("âš  ä½¿ç”¨CPUè®­ç»ƒï¼ˆå»ºè®®ä½¿ç”¨GPUï¼‰")
        return device

    def load_models(self):
        """åŠ è½½æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ï¼ˆå›¾åƒåˆ†ç±»ï¼‰"""
        print(f"\n{'='*60}")
        print("ğŸ“š åŠ è½½å›¾åƒåˆ†ç±»æ¨¡å‹...")
        print(f"{'='*60}")

        # åŠ è½½å›¾åƒå¤„ç†å™¨
        print(f"æ­£åœ¨åŠ è½½å›¾åƒå¤„ç†å™¨: {self.config.teacher_path}")
        self.image_processor = AutoImageProcessor.from_pretrained(
            self.config.teacher_path,
            trust_remote_code=True
        )

        # ç¡®å®šç±»åˆ«æ•°ï¼ˆä»é…ç½®æˆ–é»˜è®¤ï¼‰
        # TODO: ä»æ•°æ®åº“è¯»å–å®é™…çš„ç±»åˆ«æ•°
        num_classes = getattr(self.config, 'num_classes', 10)  # é»˜è®¤10ä¸ªç±»åˆ«
        print(f"ç±»åˆ«æ•°: {num_classes}")

        # åŠ è½½æ•™å¸ˆæ¨¡å‹ï¼ˆå›¾åƒåˆ†ç±»ï¼‰
        print(f"æ­£åœ¨åŠ è½½æ•™å¸ˆæ¨¡å‹: {self.config.teacher_path}")
        teacher_config = AutoConfig.from_pretrained(self.config.teacher_path)
        teacher_config.num_labels = num_classes

        self.teacher_model = AutoModelForImageClassification.from_pretrained(
            self.config.teacher_path,
            config=teacher_config,
            trust_remote_code=True,
            ignore_mismatched_sizes=True  # å…è®¸ç±»åˆ«æ•°ä¸åŒ¹é…
        )
        self.teacher_model.to(self.device)
        self.teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹è®¾ä¸ºè¯„ä¼°æ¨¡å¼

        # å†»ç»“æ•™å¸ˆæ¨¡å‹å‚æ•°
        for param in self.teacher_model.parameters():
            param.requires_grad = False

        print(f"âœ“ æ•™å¸ˆæ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in self.teacher_model.parameters()):,}")

        # åŠ è½½å­¦ç”Ÿæ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½å­¦ç”Ÿæ¨¡å‹: {self.config.student_path or 'åŸºäºæ•™å¸ˆæ¨¡å‹åˆ›å»º'}")

        if self.config.student_path:
            # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
            student_config = AutoConfig.from_pretrained(self.config.student_path)
            student_config.num_labels = num_classes
            self.student_model = AutoModelForImageClassification.from_pretrained(
                self.config.student_path,
                config=student_config,
                ignore_mismatched_sizes=True
            )
        else:
            # åˆ›å»ºæ›´å°çš„å­¦ç”Ÿæ¨¡å‹
            student_config = AutoConfig.from_pretrained(self.config.teacher_path)
            student_config.num_labels = num_classes

            # å‡å°æ¨¡å‹å°ºå¯¸ï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´ï¼‰
            if hasattr(student_config, 'num_hidden_layers'):
                student_config.num_hidden_layers = max(6, student_config.num_hidden_layers // 2)
            if hasattr(student_config, 'hidden_size'):
                student_config.hidden_size = max(384, student_config.hidden_size // 2)
            if hasattr(student_config, 'intermediate_size'):
                student_config.intermediate_size = max(1536, student_config.intermediate_size // 2)

            self.student_model = AutoModelForImageClassification.from_config(student_config)

        # åº”ç”¨LoRAé…ç½®
        self._apply_lora()

        self.student_model.to(self.device)

        print(f"âœ“ å­¦ç”Ÿæ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in self.student_model.parameters()):,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in self.student_model.parameters() if p.requires_grad):,}")

    def _apply_lora(self):
        """åº”ç”¨LoRAé…ç½®åˆ°å­¦ç”Ÿæ¨¡å‹"""
        print(f"\nğŸ”§ åº”ç”¨LoRAé…ç½®...")
        print(f"  Rank: {self.config.lora_rank}")
        print(f"  Alpha: {self.config.lora_alpha}")
        print(f"  Dropout: {self.config.lora_dropout}")
        print(f"  Target Modules: {self.config.lora_target_modules or 'default'}")

        lora_config = LoraConfig(
            task_type=TaskType.IMAGE_CLASSIFICATION,  # å›¾åƒåˆ†ç±»ä»»åŠ¡
            inference_mode=False,
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=self.config.lora_target_modules if self.config.lora_target_modules else None,
            bias=self.config.lora_bias,
        )

        self.student_model = get_peft_model(self.student_model, lora_config)
        self.student_model.print_trainable_parameters()

    def setup_optimizer(self, num_training_steps: int):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        print(f"\nâš™ï¸ è®¾ç½®ä¼˜åŒ–å™¨...")

        # é€‰æ‹©ä¼˜åŒ–å™¨
        if self.config.optimizer == "adamw":
            self.optimizer = torch.optim.AdamW(
                self.student_model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer == "adam":
            self.optimizer = torch.optim.Adam(
                self.student_model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer == "sgd":
            self.optimizer = torch.optim.SGD(
                self.student_model.parameters(),
                lr=self.config.learning_rate,
                momentum=0.9,
                weight_decay=self.config.weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.config.optimizer}")

        print(f"âœ“ ä¼˜åŒ–å™¨: {self.config.optimizer.upper()}")
        print(f"  å­¦ä¹ ç‡: {self.config.learning_rate}")
        print(f"  æƒé‡è¡°å‡: {self.config.weight_decay}")

        # é€‰æ‹©å­¦ä¹ ç‡è°ƒåº¦å™¨
        num_warmup_steps = int(0.1 * num_training_steps)  # 10% warmup

        if self.config.lr_scheduler == "cosine":
            self.scheduler = get_cosine_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=num_warmup_steps,
                num_training_steps=num_training_steps
            )
        elif self.config.lr_scheduler == "linear":
            self.scheduler = get_linear_schedule_with_warmup(
                self.optimizer,
                num_warmup_steps=num_warmup_steps,
                num_training_steps=num_training_steps
            )
        else:  # constant
            self.scheduler = None

        print(f"âœ“ å­¦ä¹ ç‡è°ƒåº¦å™¨: {self.config.lr_scheduler or 'constant'}")

    def compute_distillation_loss(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        labels: torch.Tensor
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        è®¡ç®—çŸ¥è¯†è’¸é¦æŸå¤±

        Returns:
            total_loss: æ€»æŸå¤±
            loss_dict: æŸå¤±è¯¦æƒ…å­—å…¸
        """
        # ç¡¬æ ‡ç­¾æŸå¤±ï¼ˆçœŸå®æ ‡ç­¾ï¼‰
        hard_loss = F.cross_entropy(student_logits, labels)

        # è½¯æ ‡ç­¾æŸå¤±ï¼ˆæ•™å¸ˆè¾“å‡ºï¼‰
        if self.config.distill_loss_type == "kl_div":
            # KLæ•£åº¦æŸå¤±
            soft_loss = F.kl_div(
                F.log_softmax(student_logits / self.config.temperature, dim=-1),
                F.softmax(teacher_logits / self.config.temperature, dim=-1),
                reduction='batchmean'
            ) * (self.config.temperature ** 2)
        elif self.config.distill_loss_type == "mse":
            # MSEæŸå¤±
            soft_loss = F.mse_loss(
                F.softmax(student_logits / self.config.temperature, dim=-1),
                F.softmax(teacher_logits / self.config.temperature, dim=-1)
            )
        else:
            # é»˜è®¤ä½¿ç”¨KLæ•£åº¦
            soft_loss = F.kl_div(
                F.log_softmax(student_logits / self.config.temperature, dim=-1),
                F.softmax(teacher_logits / self.config.temperature, dim=-1),
                reduction='batchmean'
            ) * (self.config.temperature ** 2)

        # ç»„åˆæŸå¤±
        total_loss = (
            self.config.hard_label_weight * hard_loss +
            self.config.soft_label_weight * soft_loss
        )

        loss_dict = {
            'total_loss': total_loss.item(),
            'hard_loss': hard_loss.item(),
            'soft_loss': soft_loss.item()
        }

        return total_loss, loss_dict

    def train_epoch(
        self,
        train_loader: DataLoader,
        epoch: int
    ) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆå›¾åƒåˆ†ç±»ï¼‰"""
        self.student_model.train()

        total_loss = 0
        total_hard_loss = 0
        total_soft_loss = 0
        correct = 0
        total = 0

        num_batches = len(train_loader)

        for batch_idx, batch in enumerate(train_loader):
            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
            pixel_values = batch['pixel_values'].to(self.device)
            labels = batch['labels'].to(self.device)

            # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
            with torch.no_grad():
                teacher_outputs = self.teacher_model(pixel_values=pixel_values)
                teacher_logits = teacher_outputs.logits

            # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
            student_outputs = self.student_model(pixel_values=pixel_values)
            student_logits = student_outputs.logits

            # è®¡ç®—æŸå¤±
            loss, loss_dict = self.compute_distillation_loss(
                student_logits, teacher_logits, labels
            )

            # æ¢¯åº¦ç´¯ç§¯
            loss = loss / self.config.grad_accum_steps
            loss.backward()

            # æ¢¯åº¦ç´¯ç§¯åæ›´æ–°
            if (batch_idx + 1) % self.config.grad_accum_steps == 0:
                # æ¢¯åº¦è£å‰ª
                if self.config.max_grad_norm > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.student_model.parameters(),
                        self.config.max_grad_norm
                    )

                self.optimizer.step()
                if self.scheduler:
                    self.scheduler.step()
                self.optimizer.zero_grad()

            # ç»Ÿè®¡
            total_loss += loss_dict['total_loss']
            total_hard_loss += loss_dict['hard_loss']
            total_soft_loss += loss_dict['soft_loss']

            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(student_logits, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % 10 == 0:
                avg_loss = total_loss / (batch_idx + 1)
                accuracy = 100.0 * correct / total
                print(f"  Batch [{batch_idx+1}/{num_batches}] "
                      f"Loss: {avg_loss:.4f} | Acc: {accuracy:.2f}%")

        # Epochç»Ÿè®¡
        epoch_metrics = {
            'loss': total_loss / num_batches,
            'hard_loss': total_hard_loss / num_batches,
            'soft_loss': total_soft_loss / num_batches,
            'accuracy': 100.0 * correct / total
        }

        return epoch_metrics

    @torch.no_grad()
    def evaluate(self, val_loader: DataLoader) -> Dict[str, float]:
        """è¯„ä¼°æ¨¡å‹ï¼ˆå›¾åƒåˆ†ç±»ï¼‰"""
        self.student_model.eval()

        total_loss = 0
        correct = 0
        total = 0

        for batch in val_loader:
            pixel_values = batch['pixel_values'].to(self.device)
            labels = batch['labels'].to(self.device)

            # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
            outputs = self.student_model(pixel_values=pixel_values)
            logits = outputs.logits

            # è®¡ç®—æŸå¤±
            loss = F.cross_entropy(logits, labels)
            total_loss += loss.item()

            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(logits, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        metrics = {
            'val_loss': total_loss / len(val_loader),
            'val_accuracy': 100.0 * correct / total
        }

        return metrics

    def save_checkpoint(self, epoch: int, metrics: Dict[str, float]):
        """ä¿å­˜checkpoint"""
        checkpoint_dir = os.path.join(self.config.output_dir, f"checkpoint-epoch-{epoch}")
        os.makedirs(checkpoint_dir, exist_ok=True)

        # ä¿å­˜æ¨¡å‹
        self.student_model.save_pretrained(checkpoint_dir)
        self.image_processor.save_pretrained(checkpoint_dir)

        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        state = {
            'epoch': epoch,
            'metrics': metrics,
            'optimizer_state': self.optimizer.state_dict(),
        }

        if self.scheduler:
            state['scheduler_state'] = self.scheduler.state_dict()

        torch.save(state, os.path.join(checkpoint_dir, 'training_state.pt'))

        print(f"âœ“ Checkpointå·²ä¿å­˜: {checkpoint_dir}")

    def update_progress_to_backend(
        self,
        epoch: int,
        metrics: Dict[str, float]
    ):
        """å›è°ƒåç«¯APIæ›´æ–°è®­ç»ƒè¿›åº¦"""
        try:
            url = f"{self.config.api_base_url}/model-distillation/tasks/{self.config.task_id}/progress"

            params = {
                'currentEpoch': epoch,
                'accuracy': metrics.get('accuracy', 0),
                'loss': metrics.get('loss', 0)
            }

            response = requests.put(url, params=params, timeout=10)

            if response.status_code == 200:
                print(f"âœ“ è¿›åº¦å·²æ›´æ–°åˆ°åç«¯ (Epoch {epoch})")
            else:
                print(f"âš  æ›´æ–°è¿›åº¦å¤±è´¥: {response.status_code}")

        except Exception as e:
            print(f"âš  æ›´æ–°è¿›åº¦å¼‚å¸¸: {str(e)}")

    def complete_task(self, final_metrics: Dict[str, float]):
        """æ ‡è®°ä»»åŠ¡å®Œæˆ"""
        try:
            url = f"{self.config.api_base_url}/model-distillation/tasks/{self.config.task_id}/complete"
            response = requests.post(url, timeout=10)

            if response.status_code == 200:
                print(f"\nâœ“ è®­ç»ƒä»»åŠ¡å·²å®Œæˆï¼")
                print(f"  æœ€ç»ˆå‡†ç¡®ç‡: {final_metrics.get('accuracy', 0):.2f}%")
            else:
                print(f"âš  æ ‡è®°å®Œæˆå¤±è´¥: {response.status_code}")

        except Exception as e:
            print(f"âš  æ ‡è®°å®Œæˆå¼‚å¸¸: {str(e)}")

    def report_error(self, error_message: str):
        """æŠ¥å‘Šè®­ç»ƒé”™è¯¯"""
        try:
            url = f"{self.config.api_base_url}/model-distillation/tasks/{self.config.task_id}/error"
            params = {'errorMessage': error_message}
            requests.put(url, params=params, timeout=10)
        except Exception as e:
            print(f"âš  æŠ¥å‘Šé”™è¯¯å¼‚å¸¸: {str(e)}")

    def train(self):
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒä»»åŠ¡: {self.config.task_id}")
        print(f"{'='*60}")

        try:
            # åŠ è½½æ¨¡å‹
            self.load_models()

            # å‡†å¤‡æ•°æ®é›†ï¼ˆè¿™é‡Œä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼Œå®é™…éœ€è¦æ›¿æ¢ï¼‰
            print(f"\nğŸ“Š å‡†å¤‡å›¾åƒæ•°æ®é›†...")

            # æ ¹æ®é…ç½®æ„å»ºæ•°æ®é›†è·¯å¾„
            # Windowsè·¯å¾„ç¤ºä¾‹: D:/pythonProject2/datasets/cifar10
            # Linuxè·¯å¾„ç¤ºä¾‹: /data/datasets/cifar10

            # æ„å»ºè®­ç»ƒé›†è·¯å¾„
            train_dataset_path = os.path.join(self.config.datasets_root, self.config.dataset_id, "train")

            # æ„å»ºéªŒè¯é›†è·¯å¾„
            val_dataset_id = self.config.val_dataset_id or self.config.dataset_id
            val_dataset_path = os.path.join(self.config.datasets_root, val_dataset_id, "val")

            print(f"è®­ç»ƒé›†è·¯å¾„: {train_dataset_path}")
            print(f"éªŒè¯é›†è·¯å¾„: {val_dataset_path}")

            # åˆ›å»ºè®­ç»ƒé›†ï¼ˆä½¿ç”¨æ•°æ®å¢å¼ºï¼‰
            train_dataset = ImageAnnotationDataset(
                dataset_path=train_dataset_path,
                image_processor=self.image_processor,
                image_size=224,
                is_training=True
            )

            # åˆ›å»ºéªŒè¯é›†ï¼ˆä¸ä½¿ç”¨æ•°æ®å¢å¼ºï¼‰
            val_dataset = ImageAnnotationDataset(
                dataset_path=val_dataset_path,
                image_processor=self.image_processor,
                image_size=224,
                is_training=False
            )

            train_loader = DataLoader(
                train_dataset,
                batch_size=self.config.batch_size,
                shuffle=True,
                num_workers=2
            )

            val_loader = DataLoader(
                val_dataset,
                batch_size=self.config.batch_size,
                shuffle=False,
                num_workers=2
            )

            print(f"âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
            print(f"âœ“ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")

            # è®¾ç½®ä¼˜åŒ–å™¨
            num_training_steps = len(train_loader) * self.config.epochs
            self.setup_optimizer(num_training_steps)

            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(self.config.output_dir, exist_ok=True)

            # è®­ç»ƒå¾ªç¯
            print(f"\n{'='*60}")
            print(f"ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ ({self.config.epochs} epochs)")
            print(f"{'='*60}\n")

            best_accuracy = 0

            for epoch in range(1, self.config.epochs + 1):
                print(f"\nğŸ“ Epoch {epoch}/{self.config.epochs}")
                print("-" * 60)

                # è®­ç»ƒ
                train_metrics = self.train_epoch(train_loader, epoch)

                # éªŒè¯
                val_metrics = self.evaluate(val_loader)

                # åˆå¹¶æŒ‡æ ‡
                all_metrics = {**train_metrics, **val_metrics}

                # æ‰“å°epochç»“æœ
                print(f"\nğŸ“ˆ Epoch {epoch} ç»“æœ:")
                print(f"  è®­ç»ƒæŸå¤±: {train_metrics['loss']:.4f}")
                print(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_metrics['accuracy']:.2f}%")
                print(f"  éªŒè¯æŸå¤±: {val_metrics['val_loss']:.4f}")
                print(f"  éªŒè¯å‡†ç¡®ç‡: {val_metrics['val_accuracy']:.2f}%")

                # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
                if val_metrics['val_accuracy'] > best_accuracy:
                    best_accuracy = val_metrics['val_accuracy']
                    print(f"  ğŸ¯ æ–°çš„æœ€ä½³å‡†ç¡®ç‡!")

                # ä¿å­˜checkpoint
                if self.config.auto_save_checkpoint and epoch % self.config.checkpoint_interval == 0:
                    self.save_checkpoint(epoch, all_metrics)

                # å›è°ƒåç«¯æ›´æ–°è¿›åº¦
                self.update_progress_to_backend(epoch, all_metrics)

            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
            final_model_dir = os.path.join(self.config.output_dir, "final_model")
            self.student_model.save_pretrained(final_model_dir)
            self.image_processor.save_pretrained(final_model_dir)
            print(f"âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_dir}")

            # æ ‡è®°ä»»åŠ¡å®Œæˆ
            final_metrics = {'accuracy': best_accuracy}
            self.complete_task(final_metrics)

            print(f"\n{'='*60}")
            print(f"ğŸ‰ è®­ç»ƒå®Œæˆ!")
            print(f"  æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.2f}%")
            print(f"  æ¨¡å‹ä¿å­˜è·¯å¾„: {final_model_dir}")
            print(f"{'='*60}\n")

        except Exception as e:
            error_msg = f"è®­ç»ƒå¤±è´¥: {str(e)}"
            print(f"\nâŒ {error_msg}")
            import traceback
            traceback.print_exc()
            self.report_error(error_msg)
            sys.exit(1)


# ==================== ä¸»å‡½æ•° ====================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="å¤§å°æ¨¡å‹ååŒè®­ç»ƒè„šæœ¬")

    # åŸºç¡€é…ç½®
    parser.add_argument("--task_id", type=str, required=True, help="ä»»åŠ¡ID")
    parser.add_argument("--api_base_url", type=str, required=True, help="åç«¯APIåœ°å€")

    # æ¨¡å‹é…ç½®
    parser.add_argument("--teacher_model", type=str, required=True, help="æ•™å¸ˆæ¨¡å‹åç§°")
    parser.add_argument("--student_model", type=str, required=True, help="å­¦ç”Ÿæ¨¡å‹åç§°")
    parser.add_argument("--teacher_path", type=str, required=True, help="æ•™å¸ˆæ¨¡å‹è·¯å¾„")
    parser.add_argument("--student_path", type=str, default="", help="å­¦ç”Ÿæ¨¡å‹è·¯å¾„")

    # æ•°æ®é…ç½®
    parser.add_argument("--dataset_id", type=str, required=True, help="æ•°æ®é›†ID")
    parser.add_argument("--val_dataset_id", type=str, default="", help="éªŒè¯æ•°æ®é›†ID")

    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=0.0001, help="å­¦ä¹ ç‡")

    # ä¼˜åŒ–å™¨é…ç½®
    parser.add_argument("--optimizer", type=str, default="adamw", help="ä¼˜åŒ–å™¨")
    parser.add_argument("--lr_scheduler", type=str, default="cosine", help="å­¦ä¹ ç‡è°ƒåº¦å™¨")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    parser.add_argument("--grad_accum_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="æœ€å¤§æ¢¯åº¦èŒƒæ•°")

    # GPUé…ç½®
    parser.add_argument("--gpu_devices", type=str, default="0", help="GPUè®¾å¤‡")
    parser.add_argument("--auto_save_checkpoint", type=bool, default=True, help="è‡ªåŠ¨ä¿å­˜checkpoint")
    parser.add_argument("--checkpoint_interval", type=int, default=5, help="checkpointä¿å­˜é—´éš”")

    # LoRAé…ç½®
    parser.add_argument("--lora_rank", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.05, help="LoRA dropout")
    parser.add_argument("--lora_target_modules", type=str, default="", help="LoRAç›®æ ‡æ¨¡å—")
    parser.add_argument("--lora_bias", type=str, default="none", help="LoRA bias")

    # çŸ¥è¯†è’¸é¦é…ç½®
    parser.add_argument("--temperature", type=float, default=3.0, help="è’¸é¦æ¸©åº¦")
    parser.add_argument("--hard_label_weight", type=float, default=0.3, help="ç¡¬æ ‡ç­¾æƒé‡")
    parser.add_argument("--soft_label_weight", type=float, default=0.7, help="è½¯æ ‡ç­¾æƒé‡")
    parser.add_argument("--distill_loss_type", type=str, default="kl_div", help="è’¸é¦æŸå¤±ç±»å‹")

    # è¾“å‡ºé…ç½®
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºç›®å½•")

    # æ•°æ®é›†æ ¹ç›®å½•ï¼ˆæ–°å¢ï¼Œç”±åç«¯é…ç½®æ–‡ä»¶ä¼ é€’ï¼‰
    parser.add_argument("--datasets_root", type=str, required=True, help="æ•°æ®é›†æ ¹ç›®å½•")

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    args = parse_args()

    # åˆ›å»ºé…ç½®
    config = TrainingConfig(args)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DistillationTrainer(config)

    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == "__main__":
    main()
