#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†è’¸é¦æµ‹è¯•è„šæœ¬
æµ‹è¯•å¤§æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰â†’ å°æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰çš„çŸ¥è¯†è’¸é¦è®­ç»ƒæµç¨‹

æ ¸å¿ƒæµ‹è¯•ï¼š
1. æ•™å¸ˆæ¨¡å‹ï¼ˆå¤§æ¨¡å‹ï¼‰ï¼šResNet50 æˆ– Qwen2.5-VL
2. å­¦ç”Ÿæ¨¡å‹ï¼ˆå°æ¨¡å‹ï¼‰ï¼šResNet18
3. è’¸é¦æŸå¤±ï¼šKLæ•£åº¦ + äº¤å‰ç†µ
4. æ¸©åº¦å‚æ•°è°ƒèŠ‚
5. å®Œæ•´è’¸é¦è®­ç»ƒæµç¨‹

ä½œè€…ï¼šClaude Assistant
æ—¥æœŸï¼š2026-01-14
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from pathlib import Path
import time
import sys

print("=" * 80)
print("ğŸ“ çŸ¥è¯†è’¸é¦æµ‹è¯• - å¤§å°æ¨¡å‹ååŒè®­ç»ƒ")
print("=" * 80)
print()

# ==================== é…ç½® ====================
BATCH_SIZE = 64
NUM_EPOCHS = 3
LEARNING_RATE = 0.001
NUM_WORKERS = 2
TEMPERATURE = 4.0  # è’¸é¦æ¸©åº¦
ALPHA = 0.7  # è’¸é¦æŸå¤±æƒé‡ï¼ˆ1-alphaä¸ºç¡¬æ ‡ç­¾æƒé‡ï¼‰
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"ğŸ“Œ è®­ç»ƒé…ç½®")
print("-" * 80)
print(f"è®¾å¤‡: {DEVICE}")
print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
print(f"è®­ç»ƒè½®æ•°: {NUM_EPOCHS}")
print(f"å­¦ä¹ ç‡: {LEARNING_RATE}")
print(f"è’¸é¦æ¸©åº¦: {TEMPERATURE}")
print(f"è’¸é¦æƒé‡ Î±: {ALPHA}")
print()

# ==================== å‡†å¤‡æ•°æ®é›† ====================
print("ğŸ“Œ å‡†å¤‡æ•°æ®é›†")
print("-" * 80)

dataset_root = Path("./datasets")
dataset_root.mkdir(exist_ok=True)

transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.Resize(224),  # ResNetéœ€è¦224x224
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

try:
    print("æ­£åœ¨åŠ è½½CIFAR-10...")
    trainset = torchvision.datasets.CIFAR10(
        root=str(dataset_root),
        train=True,
        download=True,
        transform=transform_train
    )

    testset = torchvision.datasets.CIFAR10(
        root=str(dataset_root),
        train=False,
        download=True,
        transform=transform_test
    )

    print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")
    print(f"   è®­ç»ƒé›†: {len(trainset)} å¼ ")
    print(f"   æµ‹è¯•é›†: {len(testset)} å¼ ")

except Exception as e:
    print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    sys.exit(1)

trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)
testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

print()

# ==================== åˆ›å»ºæ•™å¸ˆæ¨¡å‹ï¼ˆå¤§æ¨¡å‹ï¼‰====================
print("ğŸ“Œ åˆ›å»ºæ•™å¸ˆæ¨¡å‹ï¼ˆå¤§æ¨¡å‹ï¼‰")
print("-" * 80)

try:
    # ä½¿ç”¨ResNet101ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼ˆä¹Ÿå¯ä»¥ç”¨Qwen2.5-VLï¼Œä½†éœ€è¦å¾ˆå¤§æ˜¾å­˜ï¼‰
    teacher_model = torchvision.models.resnet101(pretrained=False, num_classes=10)
    teacher_model = teacher_model.to(DEVICE)
    teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹è®¾ä¸ºè¯„ä¼°æ¨¡å¼

    # å†»ç»“æ•™å¸ˆæ¨¡å‹å‚æ•°ï¼ˆä¸è®­ç»ƒï¼‰
    for param in teacher_model.parameters():
        param.requires_grad = False

    teacher_params = sum(p.numel() for p in teacher_model.parameters()) / 1e6

    print(f"âœ… æ•™å¸ˆæ¨¡å‹åˆ›å»ºæˆåŠŸ: ResNet101")
    print(f"   å‚æ•°é‡: {teacher_params:.2f}M")
    print(f"   æ¨¡å¼: è¯„ä¼°æ¨¡å¼ï¼ˆå†»ç»“å‚æ•°ï¼‰")

    # é€‰é¡¹ï¼šå¦‚æœæœ‰é¢„è®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹ï¼Œå¯ä»¥åŠ è½½
    teacher_checkpoint_path = Path("./test_models/teacher_resnet101.pth")
    if teacher_checkpoint_path.exists():
        print(f"   æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹...")
        checkpoint = torch.load(teacher_checkpoint_path)
        teacher_model.load_state_dict(checkpoint['model_state_dict'])
        print(f"   âœ… é¢„è®­ç»ƒæ¨¡å‹å·²åŠ è½½")
    else:
        print(f"   âš ï¸  æœªæ‰¾åˆ°é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–")
        print(f"   æç¤º: çœŸå®åœºæ™¯ä¸­æ•™å¸ˆæ¨¡å‹åº”è¯¥æ˜¯é¢„è®­ç»ƒå¥½çš„")

except Exception as e:
    print(f"âŒ æ•™å¸ˆæ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
    sys.exit(1)

print()

# ==================== åˆ›å»ºå­¦ç”Ÿæ¨¡å‹ï¼ˆå°æ¨¡å‹ï¼‰====================
print("ğŸ“Œ åˆ›å»ºå­¦ç”Ÿæ¨¡å‹ï¼ˆå°æ¨¡å‹ï¼‰")
print("-" * 80)

try:
    # ä½¿ç”¨ResNet18ä½œä¸ºå­¦ç”Ÿæ¨¡å‹
    student_model = torchvision.models.resnet18(pretrained=False, num_classes=10)
    student_model = student_model.to(DEVICE)

    student_params = sum(p.numel() for p in student_model.parameters()) / 1e6

    print(f"âœ… å­¦ç”Ÿæ¨¡å‹åˆ›å»ºæˆåŠŸ: ResNet18")
    print(f"   å‚æ•°é‡: {student_params:.2f}M")
    print(f"   å‹ç¼©æ¯”: {teacher_params / student_params:.2f}x")

except Exception as e:
    print(f"âŒ å­¦ç”Ÿæ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
    sys.exit(1)

print()

# ==================== å®šä¹‰è’¸é¦æŸå¤± ====================
print("ğŸ“Œ å®šä¹‰è’¸é¦æŸå¤±å‡½æ•°")
print("-" * 80)

class DistillationLoss(nn.Module):
    """çŸ¥è¯†è’¸é¦æŸå¤±å‡½æ•°

    ç»“åˆä¸¤éƒ¨åˆ†ï¼š
    1. è’¸é¦æŸå¤±ï¼šå­¦ç”Ÿè¾“å‡ºä¸æ•™å¸ˆè¾“å‡ºçš„KLæ•£åº¦ï¼ˆè½¯æ ‡ç­¾ï¼‰
    2. å­¦ç”ŸæŸå¤±ï¼šå­¦ç”Ÿè¾“å‡ºä¸çœŸå®æ ‡ç­¾çš„äº¤å‰ç†µï¼ˆç¡¬æ ‡ç­¾ï¼‰
    """

    def __init__(self, temperature=4.0, alpha=0.7):
        super(DistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.kl_div = nn.KLDivLoss(reduction='batchmean')
        self.ce_loss = nn.CrossEntropyLoss()

    def forward(self, student_logits, teacher_logits, labels):
        """
        Args:
            student_logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º (batch_size, num_classes)
            teacher_logits: æ•™å¸ˆæ¨¡å‹è¾“å‡º (batch_size, num_classes)
            labels: çœŸå®æ ‡ç­¾ (batch_size,)

        Returns:
            total_loss: æ€»æŸå¤±
            distill_loss: è’¸é¦æŸå¤±
            student_loss: å­¦ç”ŸæŸå¤±
        """
        # è’¸é¦æŸå¤±ï¼šKLæ•£åº¦ï¼ˆä½¿ç”¨æ¸©åº¦è½¯åŒ–ï¼‰
        student_soft = F.log_softmax(student_logits / self.temperature, dim=1)
        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)
        distill_loss = self.kl_div(student_soft, teacher_soft) * (self.temperature ** 2)

        # å­¦ç”ŸæŸå¤±ï¼šäº¤å‰ç†µï¼ˆç¡¬æ ‡ç­¾ï¼‰
        student_loss = self.ce_loss(student_logits, labels)

        # æ€»æŸå¤±ï¼šåŠ æƒç»„åˆ
        total_loss = self.alpha * distill_loss + (1 - self.alpha) * student_loss

        return total_loss, distill_loss, student_loss

criterion = DistillationLoss(temperature=TEMPERATURE, alpha=ALPHA)
optimizer = optim.Adam(student_model.parameters(), lr=LEARNING_RATE)

print(f"âœ… è’¸é¦æŸå¤±å‡½æ•°é…ç½®å®Œæˆ")
print(f"   æ¸©åº¦å‚æ•° T: {TEMPERATURE}")
print(f"   è’¸é¦æƒé‡ Î±: {ALPHA}")
print(f"   å­¦ç”Ÿæƒé‡ (1-Î±): {1-ALPHA}")
print(f"   å…¬å¼: Loss = Î± * KL(S||T) + (1-Î±) * CE(S, y)")
print()

# ==================== è®­ç»ƒå‡½æ•° ====================
def train_one_epoch(epoch):
    """è®­ç»ƒä¸€ä¸ªepochï¼ˆçŸ¥è¯†è’¸é¦ï¼‰"""
    student_model.train()
    teacher_model.eval()

    running_total_loss = 0.0
    running_distill_loss = 0.0
    running_student_loss = 0.0
    correct = 0
    total = 0

    print(f"Epoch {epoch + 1}/{NUM_EPOCHS}")
    start_time = time.time()

    for batch_idx, (inputs, labels) in enumerate(trainloader):
        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

        # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
        with torch.no_grad():
            teacher_logits = teacher_model(inputs)

        # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        student_logits = student_model(inputs)

        # è®¡ç®—è’¸é¦æŸå¤±
        total_loss, distill_loss, student_loss = criterion(
            student_logits, teacher_logits, labels
        )

        # åå‘ä¼ æ’­
        total_loss.backward()
        optimizer.step()

        # ç»Ÿè®¡
        running_total_loss += total_loss.item()
        running_distill_loss += distill_loss.item()
        running_student_loss += student_loss.item()

        _, predicted = student_logits.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        # æ¯50ä¸ªbatchæ‰“å°ä¸€æ¬¡
        if (batch_idx + 1) % 50 == 0:
            avg_total = running_total_loss / (batch_idx + 1)
            avg_distill = running_distill_loss / (batch_idx + 1)
            avg_student = running_student_loss / (batch_idx + 1)
            acc = 100.0 * correct / total

            print(f"  [{batch_idx + 1}/{len(trainloader)}] "
                  f"Total: {avg_total:.4f} | "
                  f"Distill: {avg_distill:.4f} | "
                  f"Student: {avg_student:.4f} | "
                  f"Acc: {acc:.2f}%")

    epoch_time = time.time() - start_time
    avg_total = running_total_loss / len(trainloader)
    avg_distill = running_distill_loss / len(trainloader)
    avg_student = running_student_loss / len(trainloader)
    acc = 100.0 * correct / total

    print(f"  Epochå®Œæˆ! ç”¨æ—¶: {epoch_time:.2f}s")
    print(f"  æ€»æŸå¤±: {avg_total:.4f} | è’¸é¦æŸå¤±: {avg_distill:.4f} | "
          f"å­¦ç”ŸæŸå¤±: {avg_student:.4f} | å‡†ç¡®ç‡: {acc:.2f}%")

    return avg_total, avg_distill, avg_student, acc

# ==================== æµ‹è¯•å‡½æ•° ====================
def test():
    """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å­¦ç”Ÿæ¨¡å‹"""
    student_model.eval()
    test_loss = 0
    correct = 0
    total = 0

    ce_loss = nn.CrossEntropyLoss()

    with torch.no_grad():
        for inputs, labels in testloader:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

            outputs = student_model(inputs)
            loss = ce_loss(outputs, labels)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    avg_loss = test_loss / len(testloader)
    acc = 100.0 * correct / total

    print(f"  æµ‹è¯•é›†ç»“æœ: Loss: {avg_loss:.4f} | Acc: {acc:.2f}% ({correct}/{total})")

    return avg_loss, acc

# ==================== å¯¹æ¯”ï¼šä¸ä½¿ç”¨è’¸é¦ ====================
def train_without_distillation_one_epoch(model, optimizer, epoch):
    """ä¸ä½¿ç”¨è’¸é¦çš„æ™®é€šè®­ç»ƒï¼ˆå¯¹æ¯”ï¼‰"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    ce_loss = nn.CrossEntropyLoss()

    for batch_idx, (inputs, labels) in enumerate(trainloader):
        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = ce_loss(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    avg_loss = running_loss / len(trainloader)
    acc = 100.0 * correct / total

    return avg_loss, acc

# ==================== å¼€å§‹è®­ç»ƒ ====================
print("=" * 80)
print("ğŸ‹ï¸  å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ")
print("=" * 80)
print()

training_start = time.time()
best_acc = 0.0

try:
    for epoch in range(NUM_EPOCHS):
        # è’¸é¦è®­ç»ƒ
        train_total_loss, train_distill_loss, train_student_loss, train_acc = train_one_epoch(epoch)

        # æµ‹è¯•
        print(f"  æ­£åœ¨æµ‹è¯•...")
        test_loss, test_acc = test()

        if test_acc > best_acc:
            best_acc = test_acc
            print(f"  ğŸ‰ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")

        print()

    total_time = time.time() - training_start
    print("=" * 80)
    print(f"âœ… è’¸é¦è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time:.2f}ç§’")
    print(f"   æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%")
    print("=" * 80)

except KeyboardInterrupt:
    print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
except Exception as e:
    print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print()

# ==================== å¯¹æ¯”å®éªŒï¼ˆå¯é€‰ï¼‰====================
print("=" * 80)
print("ğŸ“Š å¯¹æ¯”å®éªŒï¼šä¸ä½¿ç”¨è’¸é¦çš„è®­ç»ƒ")
print("=" * 80)
print()

print("æ­£åœ¨åˆ›å»ºæ–°çš„ResNet18ï¼ˆä¸ä½¿ç”¨è’¸é¦ï¼‰è¿›è¡Œå¯¹æ¯”...")
baseline_model = torchvision.models.resnet18(pretrained=False, num_classes=10)
baseline_model = baseline_model.to(DEVICE)
baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=LEARNING_RATE)

print("è®­ç»ƒ1ä¸ªepochè¿›è¡Œå¯¹æ¯”...")
baseline_train_loss, baseline_train_acc = train_without_distillation_one_epoch(
    baseline_model, baseline_optimizer, 0
)
baseline_test_loss, baseline_test_acc = test()

print()
print("-" * 80)
print("ğŸ” å¯¹æ¯”ç»“æœ")
print("-" * 80)
print(f"{'æ–¹æ³•':<20s} {'è®­ç»ƒå‡†ç¡®ç‡':<15s} {'æµ‹è¯•å‡†ç¡®ç‡':<15s}")
print("-" * 80)
print(f"{'çŸ¥è¯†è’¸é¦':<20s} {train_acc:<15.2f} {test_acc:<15.2f}")
print(f"{'æ™®é€šè®­ç»ƒï¼ˆåŸºçº¿ï¼‰':<20s} {baseline_train_acc:<15.2f} {baseline_test_acc:<15.2f}")
print("-" * 80)

if test_acc > baseline_test_acc:
    improvement = test_acc - baseline_test_acc
    print(f"âœ… çŸ¥è¯†è’¸é¦æå‡: +{improvement:.2f}%")
else:
    print(f"âš ï¸  åŸºçº¿æ›´å¥½ï¼ˆå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒè½®æ•°æˆ–è°ƒæ•´è¶…å‚æ•°ï¼‰")

print()

# ==================== ä¿å­˜æ¨¡å‹ ====================
print("ğŸ“Œ ä¿å­˜è’¸é¦åçš„å­¦ç”Ÿæ¨¡å‹")
print("-" * 80)

model_save_dir = Path("./test_models")
model_save_dir.mkdir(exist_ok=True)

try:
    student_path = model_save_dir / "student_resnet18_distilled.pth"
    torch.save({
        'epoch': NUM_EPOCHS,
        'model_state_dict': student_model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'test_acc': test_acc,
        'best_acc': best_acc,
        'temperature': TEMPERATURE,
        'alpha': ALPHA,
    }, student_path)
    print(f"âœ… å­¦ç”Ÿæ¨¡å‹å·²ä¿å­˜: {student_path}")
    print(f"   æ–‡ä»¶å¤§å°: {student_path.stat().st_size / 1024 / 1024:.2f} MB")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")

except Exception as e:
    print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")

print()

# ==================== æ€»ç»“ ====================
print("=" * 80)
print("ğŸ“Š çŸ¥è¯†è’¸é¦æµ‹è¯•æ€»ç»“")
print("=" * 80)
print()

print("âœ… æµ‹è¯•å®Œæˆçš„é¡¹ç›®:")
print("   - æ•™å¸ˆæ¨¡å‹ï¼ˆResNet101ï¼‰åˆ›å»ºå’Œæ¨ç†")
print("   - å­¦ç”Ÿæ¨¡å‹ï¼ˆResNet18ï¼‰åˆ›å»º")
print("   - è’¸é¦æŸå¤±è®¡ç®—ï¼ˆKLæ•£åº¦ + äº¤å‰ç†µï¼‰")
print("   - å®Œæ•´è’¸é¦è®­ç»ƒæµç¨‹")
print("   - æ¨¡å‹ä¿å­˜")
print("   - ä¸åŸºçº¿å¯¹æ¯”")
print()

print("ğŸ“ çŸ¥è¯†è’¸é¦æ ¸å¿ƒæ¦‚å¿µéªŒè¯:")
print(f"   âœ… å¤§æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰å‚æ•°é‡: {teacher_params:.2f}M")
print(f"   âœ… å°æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰å‚æ•°é‡: {student_params:.2f}M")
print(f"   âœ… æ¨¡å‹å‹ç¼©æ¯”: {teacher_params / student_params:.2f}x")
print(f"   âœ… è½¯æ ‡ç­¾æ¸©åº¦è°ƒèŠ‚: T={TEMPERATURE}")
print(f"   âœ… æŸå¤±åŠ æƒå¹³è¡¡: Î±={ALPHA}")
print()

if torch.cuda.is_available():
    print("ğŸ’¡ GPUä½¿ç”¨æƒ…å†µ:")
    print(f"   - GPU: {torch.cuda.get_device_name(0)}")
    print(f"   - æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
    print(f"   - æ˜¾å­˜å³°å€¼: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB")
else:
    print("âš ï¸  æœªä½¿ç”¨GPUåŠ é€Ÿ")

print()

print("ğŸ“ çœŸå®åº”ç”¨ä¸­çš„æ”¹è¿›æ–¹å‘:")
print("   1. ä½¿ç”¨æ›´å¼ºçš„æ•™å¸ˆæ¨¡å‹ï¼ˆResNet152, ViT-Large, Qwen2.5-VLï¼‰")
print("   2. æ•™å¸ˆæ¨¡å‹åº”è¯¥æ˜¯é¢„è®­ç»ƒå¥½çš„ï¼ˆaccuracy > 95%ï¼‰")
print("   3. å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆ50-200 epochsï¼‰")
print("   4. ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†ï¼ˆImageNetï¼‰")
print("   5. è°ƒæ•´æ¸©åº¦å’Œalphaå‚æ•°è·å¾—æœ€ä½³æ•ˆæœ")
print("   6. å¯ä»¥åŠ å…¥ä¸­é—´å±‚ç‰¹å¾è’¸é¦ï¼ˆFeature Distillationï¼‰")
print()

print("ğŸš€ ä¸‹ä¸€æ­¥:")
print("   1. å¦‚æœæ­¤æµ‹è¯•é€šè¿‡ï¼Œè¯´æ˜çŸ¥è¯†è’¸é¦æµç¨‹æ­£ç¡®")
print("   2. å¯ä»¥è®­ç»ƒä¸€ä¸ªå¼ºæ•™å¸ˆæ¨¡å‹ï¼ˆé«˜å‡†ç¡®ç‡ï¼‰")
print("   3. ç„¶åç”¨å¼ºæ•™å¸ˆæ¨¡å‹è’¸é¦åˆ°å¤šä¸ªå°æ¨¡å‹")
print("   4. éƒ¨ç½²å®Œæ•´ç³»ç»Ÿå¹¶åœ¨å‰ç«¯ç•Œé¢é…ç½®è’¸é¦ä»»åŠ¡")
print()

print("=" * 80)
print("æµ‹è¯•å®Œæˆï¼")
print("=" * 80)
