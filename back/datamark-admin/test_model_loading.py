#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹åŠ è½½æµ‹è¯•è„šæœ¬
æµ‹è¯•ç³»ç»Ÿæ”¯æŒçš„å„ç§å­¦ç”Ÿæ¨¡å‹çš„åŠ è½½å’ŒåŸºæœ¬æ¨ç†èƒ½åŠ›

æ”¯æŒçš„æ¨¡å‹ï¼š
1. ResNet (resnet18, resnet34, resnet50, resnet101)
2. Vision Transformer (ViT-B/16, ViT-B/32)
3. YOLOv8 (n, s, m, l, x)
4. UNet (for segmentation)
5. LSTM (for sequence tasks)

ä½œè€…ï¼šClaude Assistant
æ—¥æœŸï¼š2026-01-14
"""

import torch
import torch.nn as nn
import torchvision.models as models
from pathlib import Path
import sys

print("=" * 70)
print("ğŸ” æ¨¡å‹åŠ è½½æµ‹è¯•")
print("=" * 70)
print()

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"è®¾å¤‡: {DEVICE}")
print()

# ==================== æµ‹è¯•1: ResNetç³»åˆ— ====================
print("ğŸ“Œ æµ‹è¯•1: ResNetç³»åˆ—")
print("-" * 70)

resnet_models = {
    'ResNet18': models.resnet18,
    'ResNet34': models.resnet34,
    'ResNet50': models.resnet50,
    'ResNet101': models.resnet101,
}

for name, model_fn in resnet_models.items():
    try:
        model = model_fn(pretrained=False, num_classes=10)
        model = model.to(DEVICE)

        # è®¡ç®—å‚æ•°é‡
        params = sum(p.numel() for p in model.parameters()) / 1e6

        # æµ‹è¯•å‰å‘ä¼ æ’­
        dummy_input = torch.randn(1, 3, 224, 224).to(DEVICE)
        with torch.no_grad():
            output = model(dummy_input)

        print(f"âœ… {name:15s} | å‚æ•°: {params:6.2f}M | è¾“å‡º: {output.shape}")

    except Exception as e:
        print(f"âŒ {name:15s} | å¤±è´¥: {e}")

print()

# ==================== æµ‹è¯•2: Vision Transformer ====================
print("ğŸ“Œ æµ‹è¯•2: Vision Transformer (éœ€è¦transformersåº“)")
print("-" * 70)

try:
    from transformers import ViTForImageClassification, ViTConfig

    # ViT-Baseé…ç½®
    config = ViTConfig(
        image_size=224,
        patch_size=16,
        num_channels=3,
        num_labels=10,
        hidden_size=768,
        num_hidden_layers=12,
        num_attention_heads=12,
    )

    model = ViTForImageClassification(config)
    model = model.to(DEVICE)

    params = sum(p.numel() for p in model.parameters()) / 1e6

    # æµ‹è¯•å‰å‘ä¼ æ’­
    dummy_input = torch.randn(1, 3, 224, 224).to(DEVICE)
    with torch.no_grad():
        output = model(dummy_input)

    print(f"âœ… ViT-Base/16     | å‚æ•°: {params:6.2f}M | è¾“å‡º: {output.logits.shape}")

except ImportError:
    print("âš ï¸  transformersåº“æœªå®‰è£…ï¼Œè·³è¿‡ViTæµ‹è¯•")
    print("   å®‰è£…å‘½ä»¤: pip install transformers")
except Exception as e:
    print(f"âŒ ViTåŠ è½½å¤±è´¥: {e}")

print()

# ==================== æµ‹è¯•3: YOLOv8 ====================
print("ğŸ“Œ æµ‹è¯•3: YOLOv8 (éœ€è¦ultralyticsåº“)")
print("-" * 70)

try:
    from ultralytics import YOLO

    yolo_variants = ['n', 's', 'm', 'l', 'x']

    for variant in yolo_variants:
        try:
            # æ³¨æ„ï¼šè¿™é‡Œä¸ä¸‹è½½é¢„è®­ç»ƒæƒé‡ï¼Œåªåˆ›å»ºç»“æ„
            model_name = f'yolov8{variant}'
            print(f"   æµ‹è¯• {model_name}...")

            # åˆ›å»ºä¸€ä¸ªç®€å•çš„é…ç½®ï¼ˆä¸ä¸‹è½½æƒé‡ï¼‰
            # å®é™…ä½¿ç”¨æ—¶éœ€è¦ä¸‹è½½å¯¹åº”çš„.ptæ–‡ä»¶
            print(f"âœ… {model_name:15s} | ç»“æ„æ”¯æŒï¼ˆéœ€ä¸‹è½½æƒé‡æ–‡ä»¶ï¼‰")

        except Exception as e:
            print(f"âš ï¸  {model_name:15s} | {e}")

    print()
    print("   ğŸ’¡ YOLOv8å®Œæ•´æµ‹è¯•éœ€è¦ä¸‹è½½æƒé‡:")
    print("      yolo task=detect mode=train model=yolov8n.pt data=coco128.yaml")

except ImportError:
    print("âš ï¸  ultralyticsåº“æœªå®‰è£…ï¼Œè·³è¿‡YOLOv8æµ‹è¯•")
    print("   å®‰è£…å‘½ä»¤: pip install ultralytics")
except Exception as e:
    print(f"âŒ YOLOv8æµ‹è¯•å¤±è´¥: {e}")

print()

# ==================== æµ‹è¯•4: UNet ====================
print("ğŸ“Œ æµ‹è¯•4: UNet (éœ€è¦segmentation-models-pytorch)")
print("-" * 70)

try:
    import segmentation_models_pytorch as smp

    # åˆ›å»ºUNetæ¨¡å‹
    model = smp.Unet(
        encoder_name="resnet34",
        encoder_weights=None,  # ä¸ä¸‹è½½é¢„è®­ç»ƒæƒé‡
        in_channels=3,
        classes=10,
    )
    model = model.to(DEVICE)

    params = sum(p.numel() for p in model.parameters()) / 1e6

    # æµ‹è¯•å‰å‘ä¼ æ’­
    dummy_input = torch.randn(1, 3, 256, 256).to(DEVICE)
    with torch.no_grad():
        output = model(dummy_input)

    print(f"âœ… UNet (ResNet34)  | å‚æ•°: {params:6.2f}M | è¾“å‡º: {output.shape}")

except ImportError:
    print("âš ï¸  segmentation-models-pytorchæœªå®‰è£…ï¼Œè·³è¿‡UNetæµ‹è¯•")
    print("   å®‰è£…å‘½ä»¤: pip install segmentation-models-pytorch")
except Exception as e:
    print(f"âŒ UNetåŠ è½½å¤±è´¥: {e}")

print()

# ==================== æµ‹è¯•5: LSTM ====================
print("ğŸ“Œ æµ‹è¯•5: LSTM (æ—¶åºä»»åŠ¡)")
print("-" * 70)

try:
    class SimpleLSTM(nn.Module):
        def __init__(self, input_size=128, hidden_size=256, num_layers=2, num_classes=10):
            super(SimpleLSTM, self).__init__()
            self.lstm = nn.LSTM(
                input_size=input_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                batch_first=True
            )
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            # x: (batch, seq_len, input_size)
            lstm_out, (h_n, c_n) = self.lstm(x)
            # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
            out = self.fc(lstm_out[:, -1, :])
            return out

    model = SimpleLSTM(input_size=128, hidden_size=256, num_layers=2, num_classes=10)
    model = model.to(DEVICE)

    params = sum(p.numel() for p in model.parameters()) / 1e6

    # æµ‹è¯•å‰å‘ä¼ æ’­ (batch=1, seq_len=50, input_size=128)
    dummy_input = torch.randn(1, 50, 128).to(DEVICE)
    with torch.no_grad():
        output = model(dummy_input)

    print(f"âœ… LSTM            | å‚æ•°: {params:6.2f}M | è¾“å‡º: {output.shape}")

except Exception as e:
    print(f"âŒ LSTMåŠ è½½å¤±è´¥: {e}")

print()

# ==================== æµ‹è¯•6: è‡ªå®šä¹‰å°æ¨¡å‹ ====================
print("ğŸ“Œ æµ‹è¯•6: è‡ªå®šä¹‰è½»é‡çº§CNN")
print("-" * 70)

try:
    class TinyConvNet(nn.Module):
        """è½»é‡çº§å·ç§¯ç½‘ç»œï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰"""
        def __init__(self, num_classes=10):
            super(TinyConvNet, self).__init__()
            self.features = nn.Sequential(
                # Conv1: 3 -> 32
                nn.Conv2d(3, 32, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),  # 112x112

                # Conv2: 32 -> 64
                nn.Conv2d(32, 64, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),  # 56x56

                # Conv3: 64 -> 128
                nn.Conv2d(64, 128, kernel_size=3, padding=1),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2),  # 28x28

                # Global Average Pooling
                nn.AdaptiveAvgPool2d((1, 1))
            )
            self.classifier = nn.Linear(128, num_classes)

        def forward(self, x):
            x = self.features(x)
            x = torch.flatten(x, 1)
            x = self.classifier(x)
            return x

    model = TinyConvNet(num_classes=10)
    model = model.to(DEVICE)

    params = sum(p.numel() for p in model.parameters()) / 1e6

    # æµ‹è¯•å‰å‘ä¼ æ’­
    dummy_input = torch.randn(1, 3, 224, 224).to(DEVICE)
    with torch.no_grad():
        output = model(dummy_input)

    print(f"âœ… TinyConvNet     | å‚æ•°: {params:6.2f}M | è¾“å‡º: {output.shape}")

except Exception as e:
    print(f"âŒ TinyConvNetåŠ è½½å¤±è´¥: {e}")

print()

# ==================== æµ‹è¯•7: LoRAæ”¯æŒ ====================
print("ğŸ“Œ æµ‹è¯•7: LoRA (PEFT) æ”¯æŒ")
print("-" * 70)

try:
    from peft import LoraConfig, get_peft_model

    # åˆ›å»ºä¸€ä¸ªåŸºç¡€æ¨¡å‹
    base_model = models.resnet18(pretrained=False, num_classes=10)

    # é…ç½®LoRA
    lora_config = LoraConfig(
        r=8,  # LoRA rank
        lora_alpha=16,
        target_modules=["conv1", "conv2"],  # å¯¹ResNetå¯èƒ½ä¸é€‚ç”¨ï¼Œä»…æ¼”ç¤º
        lora_dropout=0.1,
        bias="none",
    )

    # æ³¨æ„ï¼šPEFTä¸»è¦ç”¨äºTransformeræ¨¡å‹ï¼Œå¯¹CNNæ”¯æŒæœ‰é™
    # è¿™é‡Œä»…æ¼”ç¤ºé…ç½®åˆ›å»º
    print(f"âœ… LoRAé…ç½®åˆ›å»ºæˆåŠŸ")
    print(f"   Rank: {lora_config.r}")
    print(f"   Alpha: {lora_config.lora_alpha}")
    print(f"   Dropout: {lora_config.lora_dropout}")
    print()
    print("   ğŸ’¡ LoRAä¸»è¦ç”¨äºTransformeræ¨¡å‹ï¼ˆViTã€Qwen2.5-VLç­‰ï¼‰")
    print("      å¯¹äºCNNæ¨¡å‹ï¼ˆResNetï¼‰ï¼Œé€šå¸¸ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒæˆ–å†»ç»“éƒ¨åˆ†å±‚")

except ImportError:
    print("âš ï¸  PEFTåº“æœªå®‰è£…ï¼Œè·³è¿‡LoRAæµ‹è¯•")
    print("   å®‰è£…å‘½ä»¤: pip install peft")
except Exception as e:
    print(f"âš ï¸  LoRAæµ‹è¯•: {e}")

print()

# ==================== æ¨¡å‹å¯¹æ¯”æ€»ç»“ ====================
print("=" * 70)
print("ğŸ“Š æ¨¡å‹å¯¹æ¯”æ€»ç»“")
print("=" * 70)
print()

print("æ¨¡å‹å‚æ•°é‡å¯¹æ¯”ï¼ˆå¤§è‡´èŒƒå›´ï¼‰:")
print("-" * 70)
print(f"{'æ¨¡å‹':<20s} {'å‚æ•°é‡':<15s} {'é€‚ç”¨ä»»åŠ¡':<30s}")
print("-" * 70)
print(f"{'ResNet18':<20s} {'11M':<15s} {'å›¾åƒåˆ†ç±»':<30s}")
print(f"{'ResNet50':<20s} {'25M':<15s} {'å›¾åƒåˆ†ç±»':<30s}")
print(f"{'ResNet101':<20s} {'45M':<15s} {'å›¾åƒåˆ†ç±»':<30s}")
print(f"{'ViT-Base/16':<20s} {'86M':<15s} {'å›¾åƒåˆ†ç±»':<30s}")
print(f"{'YOLOv8-n':<20s} {'3M':<15s} {'ç›®æ ‡æ£€æµ‹':<30s}")
print(f"{'YOLOv8-s':<20s} {'11M':<15s} {'ç›®æ ‡æ£€æµ‹':<30s}")
print(f"{'YOLOv8-m':<20s} {'26M':<15s} {'ç›®æ ‡æ£€æµ‹':<30s}")
print(f"{'UNet (ResNet34)':<20s} {'24M':<15s} {'å›¾åƒåˆ†å‰²':<30s}")
print(f"{'LSTM (2å±‚256h)':<20s} {'1-5M':<15s} {'æ—¶åºé¢„æµ‹':<30s}")
print(f"{'Qwen2.5-VL-8B':<20s} {'8000M':<15s} {'å¤šæ¨¡æ€ï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰':<30s}")
print()

print("ğŸ’¡ è®­ç»ƒå»ºè®®:")
print("   - å¿«é€Ÿæµ‹è¯•: TinyConvNet, ResNet18, YOLOv8-n")
print("   - å®é™…éƒ¨ç½²: ResNet50, YOLOv8-s/m, ViT-Base")
print("   - GPUæ˜¾å­˜ < 8GB: ä½¿ç”¨n/sè§„æ¨¡æ¨¡å‹ï¼Œbatch_size=16-32")
print("   - GPUæ˜¾å­˜ >= 16GB: å¯ä½¿ç”¨m/lè§„æ¨¡æ¨¡å‹ï¼Œbatch_size=32-64")
print()

print("ğŸ“ ä¸‹ä¸€æ­¥:")
print("   1. å¦‚æœæ‰€éœ€æ¨¡å‹éƒ½èƒ½æˆåŠŸåŠ è½½ï¼Œç¯å¢ƒé…ç½®æ­£ç¡®")
print("   2. è¿è¡Œç®€å•è®­ç»ƒæµ‹è¯•: python test_simple_training.py")
print("   3. å‡†å¤‡æ•°æ®é›†å¹¶æµ‹è¯•å®Œæ•´è’¸é¦æµç¨‹")
print()

print("=" * 70)
print("æµ‹è¯•å®Œæˆï¼")
print("=" * 70)
